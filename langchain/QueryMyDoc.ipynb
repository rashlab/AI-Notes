{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcc8bb1c",
   "metadata": {},
   "source": [
    "### QueryMyDoc.ipynb\n",
    "Q&A your own documents using LangChain, GPT and Chroma DB as a vectorstrore\n",
    "    \n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rashlab/AI-Notes/blob/main/langchain/QueryMyDoc.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90352af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5d597ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f92d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "c:\\Users\\BASEMENT\\.conda\\envs\\GPT\\lib\\site-packages\\langchain\\llms\\openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\BASEMENT\\.conda\\envs\\GPT\\lib\\site-packages\\langchain\\llms\\openai.py:608: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# download the text file we will use - state_of_the_union.txt\n",
    "input_file = 'state_of_the_union.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    data_url = 'https://github.com/rashlab/AI-Notes/raw/main/langchain/state_of_the_union.txt'\n",
    "    with open(input_file, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "loader = TextLoader(input_file)\n",
    "\n",
    "# Splitting the documents into chunks\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# using OpenAI embeddings, and create a vectorstore using Chroma DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# create the vectorstore index\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# we will compare two models - the 3.5 Turbo and the Davinci 003 \n",
    "# we make sure to use temperature=0.0, so that the model will not add any noise to the output\n",
    "llm_davinci003 = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "llm_35Turbo = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# create the QA chains - one for each model\n",
    "# first we create a pair of QA chains w/o sources, and then a pair of QA chains with sources\n",
    "# QA chains with sources will return the source document, but internally langchain also uses different prompt templates \n",
    "qa_davinci003 = RetrievalQA.from_chain_type(llm_davinci003, chain_type=\"stuff\", retriever=retriever)\n",
    "qa_35Turbo = RetrievalQA.from_chain_type(llm_35Turbo, chain_type=\"stuff\", retriever=retriever)\n",
    "qaWithSource_davinci003 = RetrievalQAWithSourcesChain.from_chain_type(llm_davinci003, chain_type=\"stuff\", retriever=retriever)\n",
    "qaWithSource_35Turbo = RetrievalQAWithSourcesChain.from_chain_type(llm_35Turbo, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# we will use the following query, which is a bit tricky for the model...\n",
    "query = \"should we raise the corporate tax?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "095db2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, I've proposed a 15% minimum tax rate for corporations.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_davinci003.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4de4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_35Turbo.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "qaWithSource_davinci003({\"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc06bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qaWithSource_35Turbo({\"question\": query}, return_only_outputs=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
