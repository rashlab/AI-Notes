<!DOCTYPE html>
<html>
<head>
<title>Transformer.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="itransformeri"><strong><i>Transformer</i></strong></h1>
<p><font size="2">[ver. 25 March 2023] open in colab: <a href="https://colab.research.google.com/github/rashlab/AI-Notes/blob/main/Transformer/Transformer.ipynb">Transformer.ipynb</a></font></p>
<p>this notebook is based on <a href="https://github.com/karpathy/minGPT">minGPT project</a> of Andrej Karpathy (tiny Shakespeare char-level GPT example)</p>
<h3 id="a-intro"><strong>A. INTRO</strong></h3>
<p><a href="https://arxiv.org/abs/1706.03762"><i>Attention Is All You Need</i></a> (Vaswani et al., 2017) introduced the Transformer, as -</p>
<blockquote>
<p><em>a model architecture eschewing recurrence and instead <strong>relying entirely on an attention mechanism</strong> to draw global dependencies between input and output. The Transformer allows for significantly more parallelization â€¦ the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution</em></p>
</blockquote>
<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/encdec.png' width=350px>
<p>The Transformer was originally designed as a sequence-to-sequence translation model, where the <strong>encoder</strong> processes the input sequence in one language, and the <strong>decoder</strong> generates the output sequence in other language. It was designed to address the limitations of recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in sequence-to-sequence NLP tasks. The Transformer eliminates the need for recurrent layers, enabling better parallelization, which significantly speeds up the training process</p>
<h3 id="b-igpt---a-generative-pre-trained-transformeri">B. <strong><i>GPT - a Generative Pre-Trained Transformer</i></strong></h3>
<p>In 2018, OpenAI introduced GPT in <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"><i>Improving Language Understanding by Generative Pre-Training</i></a> (Radford et al., 2018).
The GPT model is based on the Transformer architecture. However, it modifies the original design by discarding the encoder and solely utilizing the decoder part. This adaptation is designed to make GPT <strong>&quot;Autoregressive&quot;</strong> in the Generative mode, i.e., predict new tokens conditioning on its own previous predictions</p>
<h5 id="there-is-an-important-distinction-between-the-working-of-gpt-in-itrainingi-mode-vs-igenerativei-mode">There is an important distinction between the working of GPT in <i><strong>Training</strong></i> mode vs. <i><strong>Generative</strong></i> mode:</h5>
<blockquote>
<p><strong>in training mode</strong>, the objective of the model is to learn the language. The model is fed large amounts of text data and its only task is to predict the next token in the sequence, given the context of the previous tokens. In training mode, GPT does not generate tokens but only predictions, which are compared to the actual tokens fed as input. it is only in this phase that the model learnable parameters are updated through backpropagation, which minimizes the loss between the model predictions (aka <i><strong>logits</strong></i>) and the actual input tokens</p>
</blockquote>
<blockquote>
<p><strong>In generative mode</strong>, the objective of the GPT model is to generate tokens using the pre-trained model in an <strong>autoregressive</strong> manner. This process begins by providing the model an initial input (prompt) as a starting point. The model predicts the next tokens given the initial prompt and continues to predict subsequent tokens <strong>conditioning on its own previous predictions</strong> in an autoregressive process. The prediction is made by converting the logits into a probability distribution using the <strong>Softmax</strong> operation, and sampling the next token from the distribution based on the probability scores. In this mode, the model's <em>weights are frozen</em> and not updated. (note: Generative mode is also known as <strong>&quot;Inference&quot;</strong> mode)</p>
</blockquote>
<h4 id="this-is-how-it-works">This is how it works:</h4>
<blockquote>
<p><strong>1. Tokenization</strong>: first, the input text data is tokenized into a sequence of tokens. In the original Transformer paper, the authors used a byte-pair encoding (BPE) tokenizer, which is similar to the WordPiece tokenizer used in BERT. In the GPT-3 paper, the authors used a byte-level BPE tokenizer, which is similar to the Byte-Pair Encoding (BPE) tokenizer used in GPT-2. In both cases, the tokenizer splits the input text into a sequence of tokens, where each token is a subword unit (e.g., a word, a character, or a subword). The tokenizer may also add special tokens to the beginning and end of the sequence</p>
</blockquote>
<blockquote>
<p><strong>2. Token Embeddings</strong>: after tokenization, each token is converted into an <strong>embedding vector</strong>. The concept of Word Embedding was introduced by Mikolov et al., 2013 in the <strong>Word2Vec</strong> paper <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"><i>Distributed Representations of Words and Phrases and their Compositionality</i></a>. In the original GPT paper, the size of the embedding vector is 768; in GPT-3, the size is 1,248. in general, the larger the embedding vector, the more information the model can capture about the language. The representation of each token by a long embedding vector aims to capture the semantic information of the token in the context of the language as a whole. The embeddings represent the inherent properties and meaning of the token based on its co-occurrence patterns and relationships with other tokens in the training data. The token embeddings are part of the model's learnable parameters, and during the training process, the embeddings are updated through backpropagation in each batch iteration.</p>
</blockquote>
<blockquote>
<p><strong>3. Positional Encoding</strong>: the embedding vectors are passed through a positional encoding layer, which adds positional information to the embedding vectors. This positional information is important for the model to understand the order of the tokens in the input sequence. The resulting matrix, containing both the embedding vectors and positional encodings, is fed as input <strong>(X)</strong> into the Transformer's first Transformer block.</p>
</blockquote>
<blockquote>
<p><strong>4. Transformer Blocks</strong> - GPT applies <strong>multiple</strong> Transformer Blocks over the embeddings of input sequences. Each Transformer Block applies the following layers in sequence:</p>
</blockquote>
<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/selfatten.png' width=200px />
<blockquote>
<p><em>4.1</em> <strong>Masked Multi-Head Attention layer</strong>: Computes self-attention weights and generates a new representation of the input sequence (more on that below)</p>
</blockquote>
<blockquote>
<p><em>4.2</em> <strong>Add &amp; Norm</strong> - Adds Residual Connection to the input to the Self-Attention Layer to its output and then apply Layer Normalization to the result</p>
</blockquote>
<blockquote>
<p><em>4.3</em> <strong>Feed-Forward layer</strong> Applies a <i>pointwise</i> feed-forward layer independently to each vector in the sequence. (the term &quot;pointwise&quot; refers to the fact that the FFN operates on each token in the input sequence independently, without considering the other tokens in the sequence; this is in contrast to the convolutional layers in CNNs, which operate on a local neighborhood of the input sequence, or the recurrent layers in RNNs, which operate on the entire input sequence at once)</p>
</blockquote>
<blockquote>
<p><em>4.4</em> <strong>Add &amp; Norm</strong> - same as in step 4.2</p>
</blockquote>
<blockquote>
<p><strong>5. The output of the Transformer blocks</strong> - the self-attention block's output represent each of the input tokens, like the input token embedding, but they capture different aspects of the token's context. The input token embedding is responsible for representing the token's general meaning in the language, while the self-attention block's output is a more refined representation of the token, which takes into account its specific contextual relationships with all other tokens in the input sequence. This output vector reflects the model's understanding of the token's role and significance within the given sequence, considering the broader context, dependencies, and interactions with other tokens</p>
</blockquote>
<blockquote>
<p><strong>6. The output of the GPT Model</strong> - the output of the last Transformer block (which contains contextual vector representations for each token in the input sequence), is passed through a Linear layer, which generates a <strong><i>logits vector</i></strong>. Each element in the logits vector is a scalar value that represents the model's unnormalized confidence for the corresponding token in the vocabulary being the next token. The size of the logits vector is equal to the size of the vocabulary. From here, the working of the model depends on its mode of operation:</p>
</blockquote>
<blockquote>
<p>6.1 in Training mode, the logits are fed into a Cross-Entropy Loss function, which computes the loss by comparing the logits to the actual tokens in the input sequence. This loss is then used to update the model learnable parameters through backpropagation (after each batch iteration).</p>
</blockquote>
<blockquote>
<p>6.2 in generative / inference mode, the logits are passed through a Softmax layer, which creates a probability distribution over the vocabulary for each token in the input sequence. The selection of the next token is made by sampling from this probability distribution, using methods such as top-k or top-p sampling, or other sampling techniques like beam search.</p>
</blockquote>
<h5 id="some-notes-on-the-above">Some notes on the above:</h5>
<blockquote>
<p><strong>the Feed-Forward layer (FFN)</strong> - the FFN consists of two linear (dense) layers with a non-linear activation function, such as GeLU (Gaussian Error Linear Units) in between. The purpose of this FFN is to introduce non-linearity into the model and combine features learned by the self-attention mechanism within the Transformer. The first linear layer of the FFN increases the dimensionality of the input (commonly X4), while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data</p>
</blockquote>
<blockquote>
<p><strong>Residual Connections</strong> - In the GPT model, residual connections are found in two places within each Transformer block: the Self-Attention layer and the Feed-Forward layer. The purpose of these residual connections is to help the model learn more efficiently, by allowing gradients to flow more easily through the network during backpropagation, and mitigating the vanishing gradient problem that can occur in deep architectures.</p>
</blockquote>
<blockquote>
<p><strong>Dropout</strong> - dropout regularization technique works by randomly &quot;dropping out&quot; or setting a fraction of the neurons to zero during training, forcing the network to learn more robust features. In the Transformer architecture and GPT, dropout is typically applied at several points: (i) after the Self-Attention layer - after computing the self-attention scores and generating a new representation of the input sequence, dropout is applied to the output before the first Add &amp; Norm; (ii) after the Feed-Forward layer - after applying the FFN to each vector in the sequence, dropout is applied to the output before the second Add &amp; Norm; and (iii) in the Multi-Head Attention - dropout can also be applied to the attention scores before they are used to compute the weighted sum of the value vectors</p>
</blockquote>
<blockquote>
<p><strong>Layer Normalization</strong> - this layer apply normalization to the output values (activations) of the self-attention layer and the output values of the FFN by the mean and variance of the these activatations. This normalization technique helps the model learn more efficiently, by allowing gradients to flow more easily through the network during backpropagation, and mitigating the vanishing gradient problem that can occur in deep architectures. In PyTorch, layer normalization can be implemented using the torch.nn.LayerNorm module, which takes the number of features (neurons) as input and applies normalization across these features. <i>(note: &quot;activations&quot; typically refers to the output values of the current layer, before the activation function is applied)</i></p>
</blockquote>
<h3 id="c-ihow-the-masked-multi-head-attention-worki">C. <strong><i>How the Masked Multi-Head Attention Work</i></strong></h3>
<blockquote>
<p>Self-Attention is the fundamental operation of the Transformer. It is designed to weigh and relate the tokens of the input sequence to better capture the relationships and dependencies between them, by computing scores for each pair of elements in the input sequence. These scores determine how much &quot;attention&quot; each element should pay to other elements in the sequence. Higher scores indicate stronger relationships between elements, while lower scores suggest weaker relationships.</p>
</blockquote>
<blockquote>
<p>The Self-Attention mechanism transforms each token, initially represented by its embedding and positional encoding vector, into three vectors: a query vector (Q), a key vector (K), and a value vector (V). This transformation is achieved by applying linear transformations, specifically by multiplying the input sequence of the embedding vectors (X) with their corresponding weight matrices (W_Q, W_K, and W_V). In PyTorch, linear transformations can be implemented using the torch.nn.Linear module, which takes the number of input features and the number of output features as input. more on this <a href="https://github.com/rashlab/AI-Notes/blob/main/nn.Linear/nn.Linear.ipynb">here</a>. (note: the authors of <i>Attention Is All You Need</i> introduced the Query/Key/Value concept, drawing an analogy to a retrieval system, which might be somewhat confusing.)</p>
</blockquote>
<p>Linear Transformation of the input sequence X to Q, K, V vectors:</p>
<pre class="hljs"><code><div>Q = X @ W_Q 
K = X @ W_K 
V = X @ W_V`
</div></code></pre>
<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/qkv_dot.png' width=500/>
<blockquote>
<p><strong>Attention Scores</strong> are computed by multiplying each Q vector with each K vector. This is done by taking the dot product between the Q matrix and the transpose of K matrix (<code>Q @ K.transpose()</code>). The result is a score matrix that represents the relationships or affinities between each token in the input sequence. These scores quantify the degree to which each token should &quot;pay attention&quot; to the others.</p>
</blockquote>
<blockquote>
<p><strong>Attention Weights</strong> Attention weights are normalized attention scores, after applying Softmax so that score values sum up to 1. The Softmax function can be sensitive to very large input values that kill the gradient and slow down learning. Since the average value of the dot product grows with the embedding dimension, it helps to scale down the dot product to stop the inputs to the Softmax function from growing too large, so we divide the dot product by the square root of the embedding dimension. So the attention weights are computed as follows:</p>
</blockquote>
<p><code>A = softmax(Q @ K.transpose() / sqrt(dim_K))</code></p>
<blockquote>
<p>The Attention Output (O) is then computed by multiplying the attention scores (A) by the value vectors (V)</p>
</blockquote>
<p><code>O (Output) = A @ V</code></p>
<blockquote>
<p>The output O is a matrix of the same shape as the input X - each output vector represents a single input token and has the same size as the embedding and positional encoding vectors</p>
</blockquote>
<blockquote>
<p><strong>In summary, the attention mechanism in GPT uses learned weight matrices (W_Q, W_K, W_V) to transform the input sequence into query, key, and value vectors, then uses the dot product of the query and key tensors to compute attention weights, which are used to compute a weighted sum of the value tensor to produce the final output.</strong></p>
</blockquote>
<h5 id="some-notes-on-the-above">Some notes on the above:</h5>
<blockquote>
<p><strong>Multi-Head Attention</strong> - In each Transformer block, the Self-Attention is conducted multiple times using Multi-Head Attention, by dividing the Q, K, and V vectors into multiple subspaces (or &quot;heads&quot;), and applying the attention mechanism independently to each subspace, and then concatenates the results back into a single vector. This approach allows the model to focus on various relationships within the data simultaneously, leading to more expressive and powerful contextual representations.</p>
</blockquote>
<blockquote>
<p><strong>Causal Attention Mask</strong> - the attention mechanism in GPT uses an attention mask to prevent the model from attending to tokens that come after the current token in the input sequence. Masking is done by creating a triangular matrix where the lower triangular part is preserved, and the upper triangular part is masked. As a result, when the model processes a given token, it can only attend to the tokens that came before it or the current token itself, but not the future tokens.This is crucial for maintaining a <strong>causal structure</strong> to preventing information leakage from future tokens during training and inference, and enforcing the autoregressive property and causal structure of the model. The attention mask triangle is applied on the Attention Scores, before the Softmax operation, by setting the values of the cells we want to mask to -infinity.</p>
</blockquote>
<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/attentionmask.png' width=680 />
<h3 id="d-icodei">D. <strong><i>Code</i></strong></h3>
<p>Following is a PyTorch implementation of a basic GPT model, based on Andrej Karpathy's <a href="https://github.com/karpathy/minGPT">minGPT project</a>. Thanks Andrej!</p>
<ul>
<li>
<p>The model is trained on 1 MB txt file of Shakespeare's writings, and after short training learns to generate new sonnets that, while nonsensical, resemble Shakespeare's style</p>
</li>
<li>
<p>The model is using a simple character-level tokenizer, and the vocabulary size is relatively small - 65 unique characters: <code>!$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</code></p>
</li>
<li>
<p>The model is configured by default to run with 6 Transformer blocks (<code>C.n_layer</code>), each with 6 masked self-attention heads (<code>C.n_head</code>), and and tokens (characters) embedding size is 192 (<code>C.n_embed</code>)</p>
</li>
<li>
<p>The model has ~1.1 million learnable parameters, and on a single NVIDIA GeForce RTX 3090 it takes about 5 min to train</p>
</li>
<li>
<p>The model is using the Adam optimizer with a learning rate of 0.0001 (<code>C.lr</code>), and the loss function is Cross Entropy Loss</p>
</li>
<li>
<p>The training loop is configured to run 5000 batch iterations (<code>C.max_iters</code>); prints out info on the loss and perplexity every 200 iterations (<code>trainer.iter_num % 200 == 0</code>); and generates a new sonnet every 1000 iterations (<code>trainer.iter_num % 1000 == 0</code>).</p>
</li>
<li>
<p>in the Generative mode, the input sequence is initialized with a given prompt (<code>context = &quot;In void of faith, &quot;</code>). The seed is set to 66 (<code>C.system.seed = 66</code>)</p>
</li>
</ul>
<p>Here are two samples of generated sonnets:</p>
<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/shakespear.png' />
<pre class="hljs"><code><div>

```python
# some imports..
import math
import torch
import numpy as np
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import time
from collections import defaultdict
import random
import platform
from IPython.display import clear_output
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># some utils..</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NewGELU</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">"""  Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415. GELU is a smooth approximation of the ReLU function """</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * x * (<span class="hljs-number">1.0</span> + torch.tanh(math.sqrt(<span class="hljs-number">2.0</span> / math.pi) * (x + <span class="hljs-number">0.044715</span> * torch.pow(x, <span class="hljs-number">3.0</span>))))
    
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_seed</span><span class="hljs-params">(seed)</span>:</span>
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CfgNode</span>:</span>
    <span class="hljs-string">""" a lightweight configuration class inspired by yacs """</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>
        self.__dict__.update(kwargs)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self._str_helper(<span class="hljs-number">0</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_str_helper</span><span class="hljs-params">(self, indent)</span>:</span>
        parts = []
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.__dict__.items():
            <span class="hljs-keyword">if</span> isinstance(v, CfgNode):
                parts.append(<span class="hljs-string">"%s:\n"</span> % k)
                parts.append(v._str_helper(indent + <span class="hljs-number">1</span>))
            <span class="hljs-keyword">else</span>:
                parts.append(<span class="hljs-string">"%s: %s\n"</span> % (k, v))
        parts = [<span class="hljs-string">' '</span> * (indent * <span class="hljs-number">4</span>) + p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> parts]
        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>.join(parts)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_dict</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">""" return a dict representation of the config """</span>
        <span class="hljs-keyword">return</span> { k: v.to_dict() <span class="hljs-keyword">if</span> isinstance(v, CfgNode) <span class="hljs-keyword">else</span> v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.__dict__.items() }

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge_from_dict</span><span class="hljs-params">(self, d)</span>:</span>
        self.__dict__.update(d)     
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CausalSelfAttention</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">''' a vanilla multi-head masked self-attention layer with a projection at the end (instead of using torch.nn.MultiheadAttention) the causal mask is applied to the attention weights to ensure that the model cannot "cheat" and look into the future when making predictions. the mask is applied to the attention weights before they are normalized with a softmax.  '''</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>        
        super().__init__()
        <span class="hljs-keyword">assert</span> config.n_embd % config.n_head == <span class="hljs-number">0</span> <span class="hljs-comment"># make sure that the number of heads evenly divides the embedding size</span>
        <span class="hljs-string">''' key, query, value projections for all heads, but in a batch. The self.c_attn is a nn.Linear module, where the input size is config.n_embd (the token embedding size) and the output size is 3 * config.n_embd. In the forward step, the output tensor will split into 3 tensors along the last dimension, each with shape (batch_size, seq_len, config.n_embd). These 3 tensors will be used as the query, key, and value vectors in the self-attention mechanism '''</span>
        self.c_attn = nn.Linear(config.n_embd, <span class="hljs-number">3</span> * config.n_embd)
        <span class="hljs-comment"># projection of the concatenated multi-head attention output back to embedding size (i.e. the output size of the self-attention layer is 3 * config.n_embd, but we want to project it back to config.n_embd, which is the size of the token embeddings)</span>
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        <span class="hljs-comment"># dropout regularization</span>
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        <span class="hljs-comment"># next is a dropout regularization to the residual connection, which is a bypass that allows the gradient to flow more easily through the network</span>
        <span class="hljs-comment"># the residual connection is the addition of the input to the output of the layer</span>
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        <span class="hljs-comment"># causal mask to ensure that attention is only applied to the left in the input sequence</span>
        self.register_buffer(<span class="hljs-string">"bias"</span>, torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, config.block_size, config.block_size))
        self.n_head = config.n_head
        self.n_embd = config.n_embd

    <span class="hljs-string">''' this is called in the forward method of the Block class. x is the input tensor of shape (batch_size, seq_len, 3 * config.n_embd) i.e. ([64, 128, 576g]) in this tiny shakespear example '''</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span> 
        B, T, C = x.size()  <span class="hljs-comment"># B=Batch size, T=block size or sequence length, i.e. the number of Tokens, C=num of Channels or embedding size</span>
        <span class="hljs-comment"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span>
        <span class="hljs-comment"># we split the x input tensor along the last dimension into 3 tensors of size (batch_size, seq_len, config.n_embd)</span>
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=<span class="hljs-number">2</span>)
        <span class="hljs-comment"># now we split the tensors along the second dimension into n_head tensors of size (batch_size, seq_len, config.n_embd // n_head)</span>
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># (B, nh, T, hs)</span>

        <span class="hljs-comment"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span>
        att = (q @ k.transpose(<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>)) * (<span class="hljs-number">1.0</span> / math.sqrt(k.size(<span class="hljs-number">-1</span>)))
        <span class="hljs-comment"># here we apply the causal mask to the attention weights, before the softmax normalization</span>
        att = att.masked_fill(self.bias[:,:,:T,:T] == <span class="hljs-number">0</span>, float(<span class="hljs-string">'-inf'</span>))
        att = F.softmax(att, dim=<span class="hljs-number">-1</span>)
        att = self.attn_dropout(att)
        y = att @ v <span class="hljs-comment"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
        y = y.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(B, T, C) <span class="hljs-comment"># re-assemble all head outputs side by side</span>

        <span class="hljs-comment"># apply residual connection and projection (linear transoformation) to the input tensor y to finalize the self-attention layer</span>
        y = self.resid_dropout(self.c_proj(y))
        <span class="hljs-keyword">return</span> y
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Block</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">''' a single block of the transformer model '''</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config) <span class="hljs-comment"># init the self-attention layer</span>
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = nn.ModuleDict(dict(
            <span class="hljs-comment"># c_fc is the first linear layer in the MLP, which expands the input to a larger vector size</span>
            <span class="hljs-comment"># Specifically, it maps from a vector of size `config.n_embd` to a larger vector of size 4 * `config.n_embd`</span>
            c_fc    = nn.Linear(config.n_embd, <span class="hljs-number">4</span> * config.n_embd),
            <span class="hljs-comment"># c_proj is the second linear layer in the MLP, which maps the output of the first linear layer back to the original size of `config.n_embd`</span>
            <span class="hljs-comment"># Specifically, it maps from a larger vector of size 4 * `config.n_embd` to a vector of size `config.n_embd`</span>
            c_proj  = nn.Linear(<span class="hljs-number">4</span> * config.n_embd, config.n_embd),
            <span class="hljs-comment"># act is the activation function used in the MLP. Here, act will apply the GELU activation function to the output of the FIRST linear layer in this MLP. The purpose of this is to introduce non-linearity to the self-attention computation. The self-attention mechanism is essentially a weighted sum of the input tokens, and without non-linearity, it may not be able to capture complex relationships between the tokens. </span>
            act = NewGELU(),
            <span class="hljs-comment"># dropout layer, which in this case will apply to the output of the second linear layer (c_proj). </span>
            dropout = nn.Dropout(config.resid_pdrop),
        ))
        <span class="hljs-comment"># assign self.mlp to 'm' so that we can use it in the forward function. </span>
        m = self.mlp
        <span class="hljs-comment"># mlpf is the forward function of the MLP, which is a composition of the three layers in the MLP: c_fc, c_proj, and act</span>
        <span class="hljs-comment"># the lambda function is a shorthand for defining a function in Python. in this case, it is equivalent to the following: </span>
        <span class="hljs-comment"># def mlpf(x):</span>
        <span class="hljs-comment">#     return m.dropout(m.c_proj(m.act(m.c_fc(x))))        </span>
        self.mlpf = <span class="hljs-keyword">lambda</span> x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) <span class="hljs-comment"># MLP forward - it is called in the forward function of the Block class</span>

    <span class="hljs-comment"># this is called in the GPT class forward function</span>
    <span class="hljs-comment"># it first applies the self-attention mechanism to the input, </span>
    <span class="hljs-comment"># and then applies the MLP to the output of the self-attention mechanism</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = x + self.attn(self.ln_1(x)) <span class="hljs-comment"># this is calling the forward method of the CausalSelfAttention class. the addition of x is the residual connection</span>
        x = x + self.mlpf(self.ln_2(x)) <span class="hljs-comment"># this is calling the mlpf function defined above. the addition of x is the residual connection</span>
        <span class="hljs-keyword">return</span> x <span class="hljs-comment"># the output of the block is the input to the next block</span>
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GPT</span><span class="hljs-params">(nn.Module)</span>:</span>   
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_default_config</span><span class="hljs-params">()</span>:</span>
        C = CfgNode()
        C.model_type = <span class="hljs-string">'gpt'</span>
        C.n_layer = <span class="hljs-number">6</span>
        C.n_head = <span class="hljs-number">6</span>
        C.n_embd =  <span class="hljs-number">192</span>
        C.vocab_size = <span class="hljs-literal">None</span>
        C.block_size = <span class="hljs-literal">None</span>
        <span class="hljs-comment"># dropout hyperparameters</span>
        C.embd_pdrop = <span class="hljs-number">0.1</span>
        C.resid_pdrop = <span class="hljs-number">0.1</span>
        C.attn_pdrop = <span class="hljs-number">0.1</span>
        <span class="hljs-keyword">return</span> C

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__()
        <span class="hljs-keyword">assert</span> config.vocab_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        <span class="hljs-keyword">assert</span> config.block_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        self.block_size = config.block_size
        type_given = config.model_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        params_given = all([config.n_layer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, config.n_head <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, config.n_embd <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>])
        
        <span class="hljs-comment"># transformer is an nn.ModuleList container, and h is a contrainer used to create a list of Block modules. wte is the token embedding layer, wpe is the position embedding layer the position embedding layer is used to encode the position of a token in the sequence. </span>
        self.transformer = nn.ModuleDict(dict(            
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.embd_pdrop),            
            h = nn.ModuleList([Block(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(config.n_layer)]), 
            ln_f = nn.LayerNorm(config.n_embd),  
        ))
        <span class="hljs-comment">#  the output of the transformer is the input to the linear layer lm_head. </span>
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=<span class="hljs-literal">False</span>)

        <span class="hljs-string">''' init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper. torch.nn.apply() applies a given function to all the modules within a module container. The method takes two arguments: module - the module container on which the function will be applied; and func - the function to apply to each module. The function func is applied recursively to each submodule within the module container. The function should take a single module as input and should return the modified module. The output of torch.nn.apply() is the modified module container.         '''</span>
        self.apply(self._init_weights)
        
        <span class="hljs-string">'''next code is initializing the weight parameters for a specific layer in the model, specifically, it is initializing the weight matrix c_proj.weight. The named_parameters() method returns an iterator over the module's named parameters along with their corresponding values. The initialization is performed using the torch.nn.init.normal_() method, which sets the initial values of the tensor with random samples from a normal distribution. This scheme is known as the Xavier initialization, which aims to keep the scale of the gradients approximately the same throughout the training process.'''</span>
        <span class="hljs-keyword">for</span> pn, p <span class="hljs-keyword">in</span> self.named_parameters():
            <span class="hljs-keyword">if</span> pn.endswith(<span class="hljs-string">'c_proj.weight'</span>):
                torch.nn.init.normal_(p, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>/math.sqrt(<span class="hljs-number">2</span> * config.n_layer))

        <span class="hljs-comment"># report number of parameters (note we don't count the decoder parameters in lm_head)</span>
        n_params = sum(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.transformer.parameters())
        print(<span class="hljs-string">"number of parameters: %.2fM"</span> % (n_params/<span class="hljs-number">1e6</span>,))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_init_weights</span><span class="hljs-params">(self, module)</span>:</span>
        <span class="hljs-keyword">if</span> isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)
            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                torch.nn.init.zeros_(module.bias)
        <span class="hljs-keyword">elif</span> isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)
        <span class="hljs-keyword">elif</span> isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">configure_optimizers</span><span class="hljs-params">(self, train_config)</span>:</span>
        <span class="hljs-string">"""  This long function is unfortunately doing something very simple and is being very defensive: We are separating out all parameters of the model into two buckets: those that will experience weight decay for regularization and those that won't (biases, and layernorm/embedding weights). We are then returning the PyTorch optimizer object. """</span>
        <span class="hljs-comment"># separate out all parameters to those that will and won't experience regularizing weight decay</span>
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear, )
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
        <span class="hljs-keyword">for</span> mn, m <span class="hljs-keyword">in</span> self.named_modules():
            <span class="hljs-keyword">for</span> pn, p <span class="hljs-keyword">in</span> m.named_parameters():
                fpn = <span class="hljs-string">'%s.%s'</span> % (mn, pn) <span class="hljs-keyword">if</span> mn <span class="hljs-keyword">else</span> pn <span class="hljs-comment"># full param name random note: because named_modules and named_parameters are recursive we will see the same tensors p many many times. but doing it this way allows us to know which parent module any tensor p belongs to...</span>
                <span class="hljs-keyword">if</span> pn.endswith(<span class="hljs-string">'bias'</span>):
                    <span class="hljs-comment"># all biases will not be decayed</span>
                    no_decay.add(fpn)
                <span class="hljs-keyword">elif</span> pn.endswith(<span class="hljs-string">'weight'</span>) <span class="hljs-keyword">and</span> isinstance(m, whitelist_weight_modules):
                    <span class="hljs-comment"># weights of whitelist modules will be weight decayed</span>
                    decay.add(fpn)
                <span class="hljs-keyword">elif</span> pn.endswith(<span class="hljs-string">'weight'</span>) <span class="hljs-keyword">and</span> isinstance(m, blacklist_weight_modules):
                    <span class="hljs-comment"># weights of blacklist modules will NOT be weight decayed</span>
                    no_decay.add(fpn)

        <span class="hljs-comment"># validate that we considered every parameter</span>
        param_dict = {pn: p <span class="hljs-keyword">for</span> pn, p <span class="hljs-keyword">in</span> self.named_parameters()}
        inter_params = decay &amp; no_decay
        union_params = decay | no_decay
        <span class="hljs-keyword">assert</span> len(inter_params) == <span class="hljs-number">0</span>, <span class="hljs-string">"parameters %s made it into both decay/no_decay sets!"</span> % (str(inter_params), )
        <span class="hljs-keyword">assert</span> len(param_dict.keys() - union_params) == <span class="hljs-number">0</span>, <span class="hljs-string">"parameters %s were not separated into either decay/no_decay set!"</span> \
                                                    % (str(param_dict.keys() - union_params), )

        <span class="hljs-comment"># create the pytorch optimizer object</span>
        optim_groups = [
            {<span class="hljs-string">"params"</span>: [param_dict[pn] <span class="hljs-keyword">for</span> pn <span class="hljs-keyword">in</span> sorted(list(decay))], <span class="hljs-string">"weight_decay"</span>: train_config.weight_decay},
            {<span class="hljs-string">"params"</span>: [param_dict[pn] <span class="hljs-keyword">for</span> pn <span class="hljs-keyword">in</span> sorted(list(no_decay))], <span class="hljs-string">"weight_decay"</span>: <span class="hljs-number">0.0</span>},
        ]
        <span class="hljs-string">''' note: we are using AdamW optimizer here, which is Adam with weight decay fix. The optimizer takes in a list of parameter groups optim_groups, which are the different subsets of model parameters that have different optimization settings such as learning rate. The optimizer will update the parameters in each group separately. '''</span>
        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)
        <span class="hljs-keyword">return</span> optimizer

    <span class="hljs-comment"># in this forward method of the GPT module class, idx is the input tensor (indices of tokens), and targets is the output tensor (also indices of tokens). For our tiny shakespeares, both are of shape (b=64, t=128) where b is the batch size and t is the sequence length (num of chars). </span>
    <span class="hljs-comment"># this forward is called by the trainer in logits, self.loss = model(x, y)</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, idx, targets=None)</span>:</span> 
        device = idx.device
        b, t = idx.size()
        <span class="hljs-keyword">assert</span> t &lt;= self.block_size, <span class="hljs-string">f"Cannot forward sequence of length <span class="hljs-subst">{t}</span>, block size is only <span class="hljs-subst">{self.block_size}</span>"</span>
        <span class="hljs-comment"># This creates a tensor containing a sequence of integers starting from 0 to t with a step size of 1. The dtype argument specifies the data type of the tensor to be long, which means the values will be 64-bit integers. The unsqueeze(0) method call adds a new dimension of size 1 at the 0-th dimension of the tensor, to make the tensor compatible with other tensors in the model that have an additional batch dimension.</span>
        pos = torch.arange(<span class="hljs-number">0</span>, t, dtype=torch.long, device=device).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># shape (1, t)</span>
        <span class="hljs-comment"># forward the GPT model itself</span>
        tok_emb = self.transformer.wte(idx) <span class="hljs-comment"># token embeddings of shape (b, t, n_embd)        </span>
        pos_emb = self.transformer.wpe(pos) <span class="hljs-comment"># position embeddings of shape (1, t, n_embd)        </span>
        <span class="hljs-comment"># tok_emb and pos_emb are the token and positional embeddings, respectively, that are summed together to produce the final input embeddings to the transformer blocks. The drop method applies a dropout regularization - it works by randomly setting a fraction of the input tensor's values to zero during each training iteration. The drop method takes a single argument x and returns a tensor with the same shape as x, but with some of its elements randomly set to zero. tok_emb and pos_emb have the same shape, and when you add them together, you are performing an element-wise addition between the corresponding elements of the two tensors</span>
        x = self.transformer.drop(tok_emb + pos_emb)        
        <span class="hljs-comment"># now we feed the embeddings into the transformer blocks contained in self.transformer.h. block(x) is a call to the forward method of the Block class </span>
        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.transformer.h:
            x = block(x)
        
        <span class="hljs-comment"># apply the final layer norm and the linear layer to get the logits. the logits are the output of the linear layer of shape (batch_size, n_embed, vocab_size), where each element of the tensor represents the predicted scores for each word in the vocabulary at each position in the input sequence.       </span>
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x)

        <span class="hljs-comment"># if we are given some desired targets also calculate the loss. </span>
        <span class="hljs-comment"># The cross_entropy function takes two arguments: the logits and the targets. The logits are the output of the linear layer, and the targets are the desired output values. In this case, logits is a tensor of shape (batch_size, seq_length, vocab_size) containing the predicted logits for each possible output token at each position in the sequence, and targets is a tensor of shape (batch_size, seq_length) containing the true (index of) target tokens for each position in the sequence. The method view(-1, logits.size(-1)) is used to reshape the logits tensor to have shape (batch_size * seq_length, vocab_size). This is necessary because the F. cross_entropy() function expects the logits to have shape (N, C) where N is the total number of predictions (i.e., batch_size * seq_length) and C is the number of classes (i.e., vocab_size). The ignore_index parameter is used to specify a token index that should be ignored in the loss calculation. In this case, -1 is specified to ignore any padding tokens in the targets tensor. The resulting loss is a scalar tensor that represents the average cross-entropy loss over all tokens in the batch.</span>
        loss = <span class="hljs-literal">None</span>               
        <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            loss = F.cross_entropy(logits.view(<span class="hljs-number">-1</span>, logits.size(<span class="hljs-number">-1</span>)), targets.view(<span class="hljs-number">-1</span>), ignore_index=<span class="hljs-number">-1</span>)
        <span class="hljs-keyword">return</span> logits, loss

<span class="hljs-meta">    @torch.no_grad() # don't track gradients for this method</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate</span><span class="hljs-params">(self, idx, max_new_tokens, temperature=<span class="hljs-number">1.0</span>, do_sample=False, top_k=None)</span>:</span>
        <span class="hljs-string">""" take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete the sequence max_new_tokens times, feeding the predictions back into the model each time. """</span>
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(max_new_tokens):
            <span class="hljs-comment"># if the sequence context is growing too long we must crop it at block_size</span>
            idx_cond = idx <span class="hljs-keyword">if</span> idx.size(<span class="hljs-number">1</span>) &lt;= self.block_size <span class="hljs-keyword">else</span> idx[:, -self.block_size:]
            <span class="hljs-comment"># forward the model to get the logits for the index in the sequence</span>
            logits, _ = self(idx_cond)
            <span class="hljs-comment"># pluck the logits at the final step and scale by desired temperature</span>
            logits = logits[:, <span class="hljs-number">-1</span>, :] / temperature
            <span class="hljs-comment"># optionally crop the logits to only the top k options</span>
            <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                v, _ = torch.topk(logits, top_k)
                logits[logits &lt; v[:, [<span class="hljs-number">-1</span>]]] = -float(<span class="hljs-string">'Inf'</span>)
            <span class="hljs-comment"># apply softmax to convert logits to (normalized) probabilities</span>
            probs = F.softmax(logits, dim=<span class="hljs-number">-1</span>)
            <span class="hljs-comment"># either sample from the distribution or take the most likely element</span>
            <span class="hljs-keyword">if</span> do_sample:
                idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)
            <span class="hljs-keyword">else</span>:
                _, idx_next = torch.topk(probs, k=<span class="hljs-number">1</span>, dim=<span class="hljs-number">-1</span>)
            <span class="hljs-comment"># append sampled index to the running sequence and continue</span>
            idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>)

        <span class="hljs-keyword">return</span> idx

</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Trainer</span>:</span>
    
<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_default_config</span><span class="hljs-params">()</span>:</span>
        CN = CfgNode
        C = CN()
        <span class="hljs-comment"># device to train on</span>
        C.device = <span class="hljs-string">'auto'</span>
        <span class="hljs-comment"># dataloder parameters</span>
        C.num_workers = <span class="hljs-number">4</span>
        <span class="hljs-comment"># optimizer parameters</span>
        C.max_iters = <span class="hljs-number">5000</span>
        C.batch_size = <span class="hljs-number">64</span>
        C.learning_rate = <span class="hljs-number">3e-4</span>
        C.betas = (<span class="hljs-number">0.9</span>, <span class="hljs-number">0.95</span>)
        C.weight_decay = <span class="hljs-number">0.1</span> <span class="hljs-comment"># only applied on matmul weights</span>
        C.grad_norm_clip = <span class="hljs-number">1.0</span>
        <span class="hljs-keyword">return</span> C

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config, model, train_dataset)</span>:</span>
        print(<span class="hljs-string">"&gt;&gt;&gt; init trainer"</span>)
        self.config = config
        self.model = model
        self.optimizer = <span class="hljs-literal">None</span>
        self.train_dataset = train_dataset
        self.callbacks = defaultdict(list)

        <span class="hljs-comment"># determine the device we'll train on</span>
        <span class="hljs-keyword">if</span> config.device == <span class="hljs-string">'auto'</span>:
            self.device = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>
        <span class="hljs-keyword">else</span>:
            self.device = config.device
        self.model = self.model.to(self.device)
        print(<span class="hljs-string">"running on device"</span>, self.device)

        <span class="hljs-comment"># variables that will be assigned to trainer class later for logging and etc</span>
        self.iter_num = <span class="hljs-number">0</span>
        self.iter_time = <span class="hljs-number">0.0</span>
        self.iter_dt = <span class="hljs-number">0.0</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add_callback</span><span class="hljs-params">(self, onevent: str, callback)</span>:</span> <span class="hljs-comment"># onevent is a string that specifies when the callback should be called. callback is a function that takes the trainer object as an argument. the callback function is added to the list of callbacks for the specified event. the callback is defined </span>
        self.callbacks[onevent].append(callback)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_callback</span><span class="hljs-params">(self, onevent: str, callback)</span>:</span>
        self.callbacks[onevent] = [callback]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trigger_callbacks</span><span class="hljs-params">(self, onevent: str)</span>:</span>
        <span class="hljs-keyword">for</span> callback <span class="hljs-keyword">in</span> self.callbacks.get(onevent, []):
            callback(self)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">(self)</span>:</span>
        model, config = self.model, self.config

        <span class="hljs-comment"># setup the optimizer</span>
        self.optimizer = model.configure_optimizers(config)

        <span class="hljs-comment"># setup the dataloader, using the train_dataset that was passed in to the Trainer class. the train_dataset is a CharDataset object that is created in chargpt.py, in which the input tokens (x) and labels (y) are stored.</span>
        train_loader = DataLoader(
            self.train_dataset,
            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=<span class="hljs-literal">True</span>, num_samples=int(<span class="hljs-number">1e10</span>)), <span class="hljs-comment"># When replacement is set to True, the sampler can sample the same sample multiple times </span>
            shuffle=<span class="hljs-literal">False</span>,
            pin_memory=<span class="hljs-literal">True</span>,
            batch_size=config.batch_size,
            num_workers= <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> platform.system() == <span class="hljs-string">'Windows'</span> <span class="hljs-keyword">else</span> config.num_workers
        )
        <span class="hljs-string">''' module.train() is a method in PyTorch's nn.Module class that sets the module to training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation modes, if they are affected, e.g. Dropout, BatchNorm, etc. '''</span>
        model.train()
        <span class="hljs-comment"># iteration counter and timer. each iteration is one batch of data</span>
        self.iter_num = <span class="hljs-number">0</span>
        self.iter_time = time.time()
        <span class="hljs-string">''' The iter() function is used to create an iterator object from a DataLoader object that can be used to get the next batch of data in the dataset. The next() function is then used on this iterator object to retrieve the next batch of data.'''</span>
        data_iter = iter(train_loader)
        
        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
            <span class="hljs-string">''' fetch the next batch (x, y) and re-init iterator if needed. The except StopIteration is used to catch the StopIteration exception that is raised by the next function when there is no more data to be returned from data_iter. If there is no more data to be returned, data_iter is reset to the beginning of the data by creating a new iterator from train_loader and the next batch of data is retrieved using next(data_iter). Finally, batch is created as a list of tensors that are transferred to the device (specified by self.device) using the to method. '''</span>
            <span class="hljs-keyword">try</span>:
                batch = next(data_iter)
            <span class="hljs-keyword">except</span> StopIteration:
                data_iter = iter(train_loader)
                batch = next(data_iter)
            batch = [t.to(self.device) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> batch]

            <span class="hljs-string">''' each batch contains two tensors, x and y. x is the input token and y is the label. x and y are of shape (batch_size, seq_len). '''</span>
            x, y = batch 
            <span class="hljs-string">''' forward the model - this will call the GPT class (model) forward() method. the loss returned is a scalar tensor that represents the average cross-entropy loss over all tokens in the batch. logits is a tensor of shape (batch_size, n_embed, vocab_size) that contains the predicted scores for each class in the output vocabulary. '''</span>
            logits, self.loss = model(x, y)

            <span class="hljs-string">'''backprop and update the parameters. model.zero_grad(set_to_none=True) is used to set the gradients of all the parameters of the model to zero before computing the gradients for the current batch.      '''</span>
            model.zero_grad(set_to_none=<span class="hljs-literal">True</span>)
            <span class="hljs-string">'''In PyTorch, calling backward() on a tensor computes the gradients of that tensor with respect to a scalar value.       '''</span>
            self.loss.backward()
            
            <span class="hljs-comment"># clip the gradients and update the parameters to avoid the issue of exploding gradients during training. </span>
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)
            
            <span class="hljs-string">''' the step() method updates the model parameters using the gradients that were computed during the backward pass, scaled by the learning rate and any other optimizer-specific scaling factors.'''</span>
            self.optimizer.step()

            self.trigger_callbacks(<span class="hljs-string">'on_batch_end'</span>)
            self.iter_num += <span class="hljs-number">1</span>
            tnow = time.time()
            self.iter_dt = tnow - self.iter_time
            self.iter_time = tnow

            <span class="hljs-comment"># termination conditions</span>
            <span class="hljs-keyword">if</span> config.max_iters <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.iter_num &gt;= config.max_iters:
                self.trigger_callbacks(<span class="hljs-string">'on_finished_training'</span>)
                <span class="hljs-keyword">break</span>
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_config</span><span class="hljs-params">()</span>:</span>
    
    CN = CfgNode
    C = CN()

    <span class="hljs-comment"># system</span>
    C.system = CN()
    C.system.seed = <span class="hljs-number">66</span>    

    <span class="hljs-comment"># data</span>
    C.data = CharDataset.get_default_config()

    <span class="hljs-comment"># model</span>
    C.model = GPT.get_default_config()
    C.model.model_type = <span class="hljs-string">'gpt-mini'</span>

    <span class="hljs-comment"># trainer</span>
    C.trainer = Trainer.get_default_config()
    C.trainer.learning_rate = <span class="hljs-number">5e-4</span> <span class="hljs-comment"># the model we're using is so small that we can go a bit faster</span>

    <span class="hljs-keyword">return</span> C

<span class="hljs-comment"># -----------------------------------------------------------------------------</span>
<span class="hljs-comment"># the Dataset class is responsible for loading the data and returning batches</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CharDataset</span><span class="hljs-params">(Dataset)</span>:</span>
    <span class="hljs-string">"""
    Emits batches of characters
    """</span>

<span class="hljs-meta">    @staticmethod</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_default_config</span><span class="hljs-params">()</span>:</span>
        CN = CfgNode
        C = CN()
        C.block_size = <span class="hljs-number">128</span>
        <span class="hljs-keyword">return</span> C

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config, data)</span>:</span>
        self.config = config
        <span class="hljs-comment"># set() is a python built-in that returns a unique list of characters</span>
        chars = sorted(list(set(data)))
        data_size, vocab_size = len(data), len(chars)
        print(<span class="hljs-string">'data has %d characters, %d unique.'</span> % (data_size, vocab_size))
        <span class="hljs-comment"># create dictionaries to convert between characters and integers (stoi = string to integer) and between integers and characters (itos = integer to string)</span>
        self.stoi = { ch:i <span class="hljs-keyword">for</span> i,ch <span class="hljs-keyword">in</span> enumerate(chars) }
        self.itos = { i:ch <span class="hljs-keyword">for</span> i,ch <span class="hljs-keyword">in</span> enumerate(chars) }
        self.vocab_size = vocab_size
        self.data = data

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_vocab_size</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.vocab_size

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_block_size</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.config.block_size

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> len(self.data) - self.config.block_size

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, idx)</span>:</span>
        <span class="hljs-comment"># grab a chunk of (block_size + 1) characters from the data</span>
        chunk = self.data[idx:idx + self.config.block_size + <span class="hljs-number">1</span>]
        <span class="hljs-comment"># encode every character to an integer</span>
        dix = [self.stoi[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> chunk]
        <span class="hljs-comment"># return as tensors</span>
        x = torch.tensor(dix[:<span class="hljs-number">-1</span>], dtype=torch.long)
        y = torch.tensor(dix[<span class="hljs-number">1</span>:], dtype=torch.long)
        <span class="hljs-keyword">return</span> x, y

<span class="hljs-comment"># -----------------------------------------------------------------------------</span>


</div></code></pre>
<h4 id="prepare-the-data">prepare the data</h4>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-comment"># download the tiny shakespeare dataset</span>
input_file = <span class="hljs-string">'input.txt'</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(input_file):
    data_url = <span class="hljs-string">'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'</span>
    <span class="hljs-keyword">with</span> open(input_file, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
        f.write(requests.get(data_url).text)
<span class="hljs-keyword">with</span> open(input_file, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
    data = f.read()
print(<span class="hljs-string">f"length of dataset in characters: <span class="hljs-subst">{len(data):,}</span>"</span>)        
</div></code></pre>
<pre><code>length of dataset in characters: 1,115,394
</code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># get all the unique characters that occur in this text</span>
chars = sorted(list(set(data)))
vocab_size = len(chars)
print(<span class="hljs-string">"all the unique characters:"</span>, <span class="hljs-string">''</span>.join(chars))
print(<span class="hljs-string">f"vocab size: <span class="hljs-subst">{vocab_size:,}</span>"</span>)
</div></code></pre>
<pre><code>all the unique characters: 
 !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
vocab size: 65
</code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># create a mapping from characters to integers</span>
stoi = { ch:i <span class="hljs-keyword">for</span> i,ch <span class="hljs-keyword">in</span> enumerate(chars) }
itos = { i:ch <span class="hljs-keyword">for</span> i,ch <span class="hljs-keyword">in</span> enumerate(chars) }
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode</span><span class="hljs-params">(s)</span>:</span>
    <span class="hljs-keyword">return</span> [stoi[c] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s] <span class="hljs-comment"># encoder: take a string, output a list of integers</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">decode</span><span class="hljs-params">(l)</span>:</span>
    <span class="hljs-string">''</span>.join([itos[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> l]) <span class="hljs-comment"># decoder: take a list of integers, output a string</span>

<span class="hljs-comment"># create the train and test splits</span>
n = len(data)
train_data = data[:int(n*<span class="hljs-number">0.9</span>)]
val_data = data[int(n*<span class="hljs-number">0.9</span>):]

<span class="hljs-comment"># encode both to integers</span>
train_ids = encode(train_data)
val_ids = encode(val_data)
print(<span class="hljs-string">f"train has <span class="hljs-subst">{len(train_ids):,}</span> tokens"</span>)
print(<span class="hljs-string">f"val has <span class="hljs-subst">{len(val_ids):,}</span> tokens"</span>)
</div></code></pre>
<pre><code>train has 1,003,854 tokens
val has 111,540 tokens
</code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># export to bin files</span>
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(<span class="hljs-string">'train.bin'</span>)
val_ids.tofile(<span class="hljs-string">'val.bin'</span>)

<span class="hljs-comment"># save the meta information as well, to help us encode/decode later</span>
meta = {
    <span class="hljs-string">'vocab_size'</span>: vocab_size,
    <span class="hljs-string">'itos'</span>: itos,
    <span class="hljs-string">'stoi'</span>: stoi,
}
<span class="hljs-keyword">with</span> open(<span class="hljs-string">'meta.pkl'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:
    pickle.dump(meta, f)
</div></code></pre>
<h4 id="start-training">start training</h4>
<pre class="hljs"><code><div><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:

    <span class="hljs-comment"># get default config and overrides from the command line, if any</span>
    config = get_config()
    set_seed(config.system.seed)

    <span class="hljs-comment"># construct the training dataset</span>
    text = open(input_file, <span class="hljs-string">'r'</span>).read() 
    train_dataset = CharDataset(config.data, text)

    <span class="hljs-comment"># construct the model</span>
    config.model.vocab_size = train_dataset.get_vocab_size()
    config.model.block_size = train_dataset.get_block_size()
    model = GPT(config.model)

    <span class="hljs-comment"># construct the trainer object</span>
    trainer = Trainer(config.trainer, model, train_dataset)

    <span class="hljs-comment"># iteration callback</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">batch_end_callback</span><span class="hljs-params">(trainer)</span>:</span>

        <span class="hljs-keyword">if</span> trainer.iter_num % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:
            print(<span class="hljs-string">f"iter_dt <span class="hljs-subst">{trainer.iter_dt * <span class="hljs-number">1000</span>:<span class="hljs-number">.2</span>f}</span>ms; iter <span class="hljs-subst">{trainer.iter_num}</span>: train loss <span class="hljs-subst">{trainer.loss.item():<span class="hljs-number">.5</span>f}</span>"</span>)

        <span class="hljs-keyword">if</span> trainer.iter_num % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:
            <span class="hljs-comment"># evaluate both the train and test score</span>
            model.eval()
            <span class="hljs-keyword">with</span> torch.no_grad():
                <span class="hljs-comment"># sample from the model...</span>
                context = <span class="hljs-string">"In void of faith, "</span>
                x = torch.tensor([train_dataset.stoi[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> context], dtype=torch.long)[<span class="hljs-literal">None</span>,...].to(trainer.device)
                y = model.generate(x, <span class="hljs-number">150</span>, temperature=<span class="hljs-number">1.0</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">10</span>)[<span class="hljs-number">0</span>]
                completion = <span class="hljs-string">''</span>.join([train_dataset.itos[int(i)] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> y])
                print(<span class="hljs-string">'\n===&gt;'</span>, completion, <span class="hljs-string">'&lt;===\n '</span>)            
            <span class="hljs-comment"># revert model to training mode</span>
            model.train()        

    <span class="hljs-comment"># finished_training_callback</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finished_training_callback</span><span class="hljs-params">(trainer)</span>:</span>
        model.eval()
        <span class="hljs-keyword">with</span> torch.no_grad():
            <span class="hljs-comment"># sample from the model...</span>
            context = <span class="hljs-string">"In void of faith, "</span>
            x = torch.tensor([train_dataset.stoi[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> context], dtype=torch.long)[<span class="hljs-literal">None</span>,...].to(trainer.device)
            y = model.generate(x, <span class="hljs-number">500</span>, temperature=<span class="hljs-number">1.0</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">10</span>)[<span class="hljs-number">0</span>]
            completion = <span class="hljs-string">''</span>.join([train_dataset.itos[int(i)] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> y])
            clear_output(wait=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># Clear the output before printing the new value    </span>
            print(<span class="hljs-string">'\n'</span>, completion, <span class="hljs-string">'\n '</span>)                
            time.sleep(<span class="hljs-number">1</span>)  <span class="hljs-comment"># Wait for 1 second  </span>

    <span class="hljs-comment"># register the callbacks</span>
    trainer.set_callback(<span class="hljs-string">'on_batch_end'</span>, batch_end_callback)
    trainer.set_callback(<span class="hljs-string">'on_finished_training'</span>, finished_training_callback)

    <span class="hljs-comment"># run the optimization</span>
    trainer.run()
</div></code></pre>
<pre><code> In void of faith, and show thee men,
That together my daughters in heaven and thy growthry,
So labour'd lineaments are those my speaks,
Strikes tongues, for an ach ancient hests,
If you'll countenance him he all all hers.
Or will we defier it?

BRUTUS:
I'll none, but betimes our flatterer pleasure
Their love, and thine saltier are their halls
After hatches their song. Come, sometimes.

CORIOLANUS:
O groans!
A dog! thousand on this disland of hair,--
We'll be some of good spare out.

CORIOLANUS:
I must confess.
Ha 
</code></pre>

</body>
</html>
