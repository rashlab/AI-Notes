{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<i>Transformer</i>**\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rashlab/AI-Notes/blob/main/Transformer/Transformer.ipynb)\n",
    "\n",
    "this notebook is based on [minGPT project](https://github.com/karpathy/minGPT) of Andrej Karpathy (tiny Shakespeare char-level GPT example) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. <i>INTRO</i>**\n",
    "[<i>Attention Is All You Need</i>](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017) introduced the Transformer, as -\n",
    "> _a model architecture eschewing recurrence and instead **relying entirely on an attention mechanism** to draw global dependencies between input and output. The Transformer allows for significantly more parallelization … the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution_\n",
    "\n",
    "<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/encdec.png' width=350px>\n",
    "\n",
    "The Transformer was originally designed as a sequence-to-sequence translation model, where the **encoder** processes the input sequence in one language, and the **decoder** generates the output sequence in other language. It was designed to address the limitations of recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in sequence-to-sequence NLP tasks. The Transformer eliminates the need for recurrent layers, enabling better parallelization, which significantly speeds up the training process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. <i>GPT - a Generative Pre-Trained Transformer</i>**\n",
    "In 2018, OpenAI introduced GPT in [<i>Improving Language Understanding by Generative Pre-Training</i>](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford et al., 2018). \n",
    "The GPT model is based on the Transformer architecture. However, it modifies the original design by discarding the encoder and solely utilizing the decoder part. This adaptation is designed to make GPT **\"Autoregressive\"** in the Generative mode, i.e., predict new tokens conditioning on its own previous predictions\n",
    "\n",
    "##### There is an important distinction between the working of GPT in <i>**Training**</i> mode vs. <i>**Generative**</i> mode:\n",
    "\n",
    "> __in training mode__, the objective of the model is to learn and understand the language. The model is fed huge amounts of text data and its task is to predict the next token in the sequence, given the context of the previous tokens. In training mode, GPT does not generate any new tokens but only predictions. In this mode, the predictions are used to calculate the 'Loss', which is the difference between the model predictions and the actual input tokens, and the loss is used to calculate the required updates to the model learnable parameters through backpropagation \n",
    "\n",
    "> **In generative (or inference) mode**, the objective of the GPT model is to generate new tokens, not only predictions. The process starts by providing a pre-trained model a prompt as an initial input, and the model starts to predict the next tokens given this initial prompt, but then the model continues to predict and generate **new** tokens conditioning on **its own** previous predictions in an <i>autoregressive</i> manner. The new tokens are generated by sampling the next token from the probability distribution generated in the prediction process, based on the probability scores. In this mode, the model's *weights are frozen* and not updated\n",
    "\n",
    "#### This is how it works:\n",
    "\n",
    "> **1. Tokenization**: first, the input text data is tokenized into a sequence of tokens. In the original Transformer paper, the authors used a byte-pair encoding (BPE) tokenizer, which is similar to the WordPiece tokenizer used in BERT. In the GPT-3 paper, the authors used a byte-level BPE tokenizer, which is similar to the Byte-Pair Encoding (BPE) tokenizer used in GPT-2. In both cases, the tokenizer splits the input text into a sequence of tokens, where each token is a subword unit (e.g., a word, a character, or a subword). The tokenizer may also add special tokens to the beginning and end of the sequence\n",
    "\n",
    "> **2. Token Embeddings**: after tokenization, each token is converted into an **embedding vector**. The concept of Word Embedding was introduced by Mikolov et al., 2013 in the **Word2Vec** paper [<i>Distributed Representations of Words and Phrases and their Compositionality</i>](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). In the original GPT paper, the size of the embedding vector is 768; in GPT-3, the size is 1,248. in general, the larger the embedding vector, the more information the model can capture about the language. The representation of each token by a long embedding vector aims to capture the semantic information of the token in the context of the language as a whole. The embeddings represent the inherent properties and meaning of the token based on its co-occurrence patterns and relationships with other tokens in the training data. The token embeddings are part of the model's learnable parameters, and during the training process, the embeddings are updated through backpropagation in each batch iteration. \n",
    "\n",
    "> **3. Positional Encoding**: the embedding vectors are passed through a positional encoding layer, which adds positional information to the embedding vectors. This positional information is important for the model to understand the order of the tokens in the input sequence. The resulting matrix, containing both the embedding vectors and positional encodings, is fed as input **(X)** into the Transformer's first Transformer block.\n",
    "\n",
    "> **4. Transformer Blocks** - GPT applies __multiple__ Transformer Blocks over the input sequences. Each Transformer Block applies the following layers in sequence:  \n",
    "\n",
    "<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/selfatten.png' width=200px />\n",
    "\n",
    ">>  4.1 **Masked Multi-Head Attention layer**: Computes self-attention weights and generates a new representation of the input sequence (see part **C.** below)\n",
    "\n",
    ">>  4.2 **Add & Norm** - Adds Residual Connection to the input to the Self-Attention Layer and then apply Layer Normalization (see note below)\n",
    "\n",
    ">>  4.3 **Feed-Forward layer** Applies a <i>pointwise</i> feed-forward layer independently to each vector in the sequence. (\"pointwise\" means that the FFN operates on each token in the input sequence independently, without considering the other tokens in the sequence; this is in contrast to the convolutional layers in CNNs, which operate on a local neighborhood of the input sequence, or the recurrent layers in RNNs, which operate on the entire input sequence at once)\n",
    "\n",
    ">>  4.4 **Add & Norm** - same as in step 4.2\n",
    "\n",
    "> **5. The output of the Transformer blocks** - the self-attention block's output represent each of the input tokens, similar to the input token embedding, but they capture different aspects of the token's context. The input token embedding is responsible for representing the token's general meaning in the language, while the output of the Transformer blocks is a more refined representation of the token, which takes into account its specific contextual relationships with all other tokens in the input sequence. This output vector reflects the model's understanding of the token's role and significance within the given sequence, considering the broader context, dependencies, and interactions with other input tokens\n",
    "\n",
    "> **6. The output of the GPT Model** - the output of the last Transformer block is passed through a Linear layer, which generates a **<i>logits vector</i>**, where each element in the logits vector is a scalar (a single numerical value) that represents the model's unnormalized confidence for the corresponding token in the vocabulary being the next token. The size of the logits vector is equal to the size of the vocabulary of the model. The logits are passed through a Softmax layer, which creates a probability distribution over the vocabulary for each token in the input sequence. From here, the working of the model depends on its mode of operation:\n",
    "\n",
    ">>    6.1 in Training mode, the Softmax normalization output is fed into a Cross-Entropy Loss function, which computes the loss by comparing the model prediction (\"what is the next token?\") to the actual next token in the input sequence. This loss is then used to update the model learnable parameters through backpropagation (after each batch iteration).\n",
    "\n",
    ">>    6.2 in Generative/Inference mode, the purpose is to generate a new token, the selection of which is made by sampling from the Softmax normalization output, using methods such as top-k or top-p sampling, or other sampling techniques like beam search. \n",
    "\n",
    "##### Some notes on the above:\n",
    "\n",
    "> **the Feed-Forward layer (FFN)** - this is a multi-layer perceptron (MLP), consisting of two linear (dense) layers with a non-linear activation function, such as GeLU (Gaussian Error Linear Units) in between. The purpose of this FFN is to introduce non-linearity into the model and combine features learned by the self-attention mechanism. The first linear layer of the FFN increases the dimensionality of the input (commonly X4), while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. \n",
    ">> Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. In recent work, researchers show that feed-forward layers in transformer-based language models have a central role in the **memory** capabilities of the model (the \"Neural Memory\") - see [Transformer Feed-Forward Layers Are Key-Value Memories (2021)](https://aclanthology.org/2021.emnlp-main.446.pdf) and [the ROME project](https://rome.baulab.info/)\n",
    "\n",
    "> **Residual Connections** - In the GPT model, residual connections are found in two places within each Transformer block: the Self-Attention layer and the Feed-Forward layer. The purpose of these residual connections is to help the model learn more efficiently, by allowing gradients to flow more easily through the network during backpropagation, and mitigating the vanishing gradient problem that can occur in deep architectures.\n",
    "\n",
    "> **Layer Normalization** - this layer apply normalization to the output values (the \"activations\") of the self-attention layer and the output values of the FFN by the mean and variance of the these activatations. This normalization technique helps the model learn more efficiently, by allowing gradients to flow more easily through the network during backpropagation, and mitigating the vanishing gradient problem that can occur in deep architectures. In PyTorch, layer normalization can be implemented using the ```torch.nn.LayerNorm``` module, which takes the number of features (neurons) as input and applies normalization across these features. <i>(note: \"activations\" typically refers to the output values of the current layer, before the activation function is applied)</i>\n",
    ">> in the original paper, the first nn.LayerNorm is applied after the attention layer (Post-LN), in other implementations the first nn.LayerNorm is applied before the attention layer (Pre-LN). Karpathy used the Pre-LN version, which is more common today (```x = x + self.attn(self.ln_1(x))```). \n",
    "\n",
    "> **Dropout** - dropout regularization technique works by randomly \"dropping out\" or setting a fraction of the neurons to zero during training, forcing the network to learn more robust features. In the Transformer architecture and GPT, dropout is typically applied at several points: (i) after the Self-Attention layer - after computing the self-attention scores and generating a new representation of the input sequence, dropout is applied to the output before the first Add & Norm; (ii) after the Feed-Forward layer - after applying the FFN to each vector in the sequence, dropout is applied to the output before the second Add & Norm; and (iii) in the Multi-Head Attention - dropout can also be applied to the attention scores before they are used to compute the weighted sum of the value vectors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. **<i>The Multi-Head Masked Attention</i>**\n",
    "\n",
    " Self-Attention is the fundamental operation of the Transformer. It is designed to weigh and relate the tokens of the input sequence to better capture the relationships and dependencies between them, by computing scores for each pair of elements in the input sequence. These scores determine how much \"attention\" each element should pay to other elements in the sequence. Higher scores indicate stronger relationships between elements, while lower scores suggest weaker relationships. \n",
    "\n",
    " The Self-Attention mechanism transforms each token, initially represented by its embedding and positional encoding vector, into three vectors: a query vector (Q), a key vector (K), and a value vector (V). This transformation is achieved by applying linear transformations, specifically by multiplying the input sequence of the embedding vectors (X) with their corresponding weight matrices (W_Q, W_K, and W_V). In PyTorch, linear transformations can be implemented using the ```torch.nn.Linear``` module, which takes the number of input features and the number of output features as input. more on this [here](https://github.com/rashlab/AI-Notes/blob/main/nn.Linear/nn.Linear.ipynb). (note: the authors of <i>Attention Is All You Need</i> introduced the Query/Key/Value concept, drawing an analogy to a retrieval system, which might be somewhat confusing.) \n",
    "\n",
    "Linear Transformation of the input sequence X to Q, K, V vectors:\n",
    "```\n",
    "Q = X @ W_Q \n",
    "K = X @ W_K \n",
    "V = X @ W_V`\n",
    "```\n",
    "\n",
    "<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/qkv_dot.png' width=500/>\n",
    "\n",
    "> **Attention Scores** are computed by multiplying each Q vector with each K vector. This is done by taking the dot product between the Q matrix and the transpose of K matrix. Since the softmax that will be applied next can be sensitive to very large input values that kill the gradient and slow down learning, we divide by the square root of the embedding dimension, so attention scores (```att```) are computed as follows: \n",
    "\n",
    "```att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))```\n",
    "    \n",
    "> **Attention Weights** are normalized attention scores, after applying Softmax so that score values sum up to 1:\n",
    "\n",
    "```att = F.softmax(att, dim=-1)```\n",
    "\n",
    "> The Attention Output (O) is then computed by multiplying the attention scores (A) by the value vectors (V) \n",
    "\n",
    "```O (Output) = A @ V``` \n",
    "\n",
    "> The output O is a matrix of the same shape as the input X - each output vector represents a single input token and has the same size as the embedding and positional encoding vectors \n",
    "\n",
    "**In summary, the attention mechanism in GPT uses learned weight matrices (W_Q, W_K, W_V) to transform the input sequence into query, key, and value vectors, then uses the dot product of the query and key tensors to compute attention weights, which are used to compute a weighted sum of the value vector to produce the output.**\n",
    "\n",
    "##### Some notes on the above:\n",
    "\n",
    "> **Multi-Head Attention** - In each Transformer block, the Self-Attention is conducted multiple times using Multi-Head Attention, by dividing the Q, K, and V vectors into multiple subspaces (or \"heads\"), and applying the attention mechanism independently to each subspace, and then concatenates the results back into a single vector. This approach allows the model to focus on various relationships within the data simultaneously, leading to more expressive and powerful contextual representations. \n",
    "\n",
    "> **Causal Attention Mask** - the attention mechanism in GPT uses an attention mask to prevent the model from attending to tokens that come after the current token in the input sequence. Masking is done by creating a triangular matrix where the lower triangular part is preserved, and the upper triangular part is masked. As a result, when the model processes a given token, it can only attend to the tokens that came before it or the current token itself, but not the future tokens.This is crucial for maintaining a **causal structure** to preventing information leakage from future tokens during training and inference, and enforcing the autoregressive property and causal structure of the model. The attention mask triangle is applied on the Attention Scores, before the Softmax operation, by setting the values of the cells we want to mask to -infinity. \n",
    "\n",
    "<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/attentionmask.png' width=680 />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D. <i>Code</i>**\n",
    "Following is a PyTorch implementation of a basic GPT model, based on Andrej Karpathy's [minGPT project](https://github.com/karpathy/minGPT). Thanks Andrej! \n",
    "\n",
    "**General**\n",
    "\n",
    "* the model is trained on ~1 MB txt file of Shakespeare's writings, and after short training learns to generate new sonnets that, while nonsensical, resemble Shakespeare's style\n",
    "* the model is using a simple character-level tokenizer, and the vocabulary size is relatively small - 65 unique characters: ```!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz```\n",
    "* the model has ~2.7 million learnable parameters, and on a single NVIDIA GeForce RTX 3090 it takes about 5 min to train\n",
    "* the loss function is **Cross Entropy Loss** (```loss = F.cross_entropy```). It measures the difference between the model's prediction distribution for the next token (derived from the softmax normalization of the logits) and the actual (true) next token in the input sequence, using negative log-likelihood. The loss is computed for each token in the input sequence, and the final loss is the average of the losses for all tokens in the sequence\n",
    "* the optimizer is AdamW (```optimizer = torch.optim.AdamW```). (<i>this is the algorithm used to update the learnable parameters during the training process, using the gradients computed via backpropagation. AdamW is a variant of Adam, which is an adaptive learning rate optimization algorithm, and it adds a Weight decay regularization technique that penalizes large weights, which helps to prevent overfitting</i>)\n",
    "\n",
    "**Class Structure:**\n",
    "* ```GPT class``` is the main class that represents the GPT model. It consists of a transformer, which is an ```nn.ModuleDict``` (a PyTorch container that inherits from the base ```nn.Module```) containing token embedding (wte), position embedding (wpe), dropout, layer normalization (ln_f), and a list of transformer Blocks (h). The forward method of this class applies the token and position embeddings to the input, and then passes the result through the transformer blocks\n",
    "\n",
    "* ```Block class``` represents a single Transformer block. It contains a layer normalization layer (ln_1), a causal self-attention layer (attn), another layer normalization layer (ln_2), and a multi-layer perceptron (MLP) module. The forward method of this class applies the self-attention mechanism and the MLP to the input in sequence\n",
    "\n",
    "* ```CausalSelfAttention class``` represents the causal self-attention mechanism in a transformer block. It consists of key, query, and value projections for all heads, as well as output projection and dropout regularization. The forward method of this class applies the self-attention mechanism to the input\n",
    "        \n",
    "* ```Trainer Class``` is responsible for managing the training process of the GPT model. It includes methods for setting up the device and configuring the optimizer. The core of the training process is implemented in the run method, which iterates through the training dataset using a DataLoader, fetches input-output pairs (x, y, rerpresenting tokens Tn and Tn+1), and performs forward and backward passes through the model. The gradients are clipped to prevent exploding gradients, and the optimizer's step method is called to update the model parameters.            \n",
    "\n",
    "\n",
    "**Configuration**    \n",
    "* embedding vector size - 192 (```C.n_embed```)     \n",
    "* number of Transformer blocks - 6 (```C.n_layer```);     \n",
    "* number of attention heads in each Transformer block - 6 (```C.n_head```);           \n",
    "* sequence length or \"block\" size (number of tokens in each input sequence) - 128 (```C.block_size```);  \n",
    "* batch size (number of training examples in each forward/backward pass) - 64 (```C.batch_size```)       \n",
    "* max number of iterations (an iteration is a run on one batch of inputs) - 5000 (```C.max_iters```);     \n",
    "* learning rate (used by AdamW optimizer as the initial rate step size at which weights are updated) - 3e-4 (= 3*10^(-4) = 0.0003) (```C.learning_rate```);\n",
    "* seed - ```C.system.seed```;  \n",
    "* logging - print out info on current loss every 200 iterations (```trainer.iter_num % 200 == 0```);    \n",
    "* sampling - generate a new sonnet every 1000 iterations (```trainer.iter_num % 1000 == 0```);       \n",
    "* initial prompt (for sampling new sonnet) - ```context = \"In void of faith, \"```;           \n",
    "\n",
    "**Results**\n",
    "\n",
    "Here's a sample sonnet generated by the model after 5000 iterations of training:\n",
    "```\n",
    "In void of faith, and show thee men,\n",
    "That together my daughters in heaven and thy growthry,\n",
    "So labour'd lineaments are those my speaks,\n",
    "Strikes tongues, for an ach ancient hests,\n",
    "If you'll countenance him he all all hers.\n",
    "Or will we defier it?\n",
    "\n",
    "BRUTUS:\n",
    "I'll none, but betimes our flatterer pleasure\n",
    "Their love, and thine saltier are their halls\n",
    "After hatches their song. Come, sometimes.\n",
    "\n",
    "CORIOLANUS:\n",
    "O groans!\n",
    "A dog! thousand on this disland of hair,--\n",
    "We'll be some of good spare out.\n",
    "\n",
    "CORIOLANUS:\n",
    "I must confess.\n",
    "Ha\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports..\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import platform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils..\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"  Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415. GELU is a smooth approximation of the ReLU function \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class CfgNode:\n",
    "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._str_helper(0)\n",
    "\n",
    "    def _str_helper(self, indent):\n",
    "        parts = []\n",
    "        for k, v in self.__dict__.items():\n",
    "            if isinstance(v, CfgNode):\n",
    "                parts.append(\"%s:\\n\" % k)\n",
    "                parts.append(v._str_helper(indent + 1))\n",
    "            else:\n",
    "                parts.append(\"%s: %s\\n\" % (k, v))\n",
    "        parts = [' ' * (indent * 4) + p for p in parts]\n",
    "        return \"\".join(parts)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\" return a dict representation of the config \"\"\"\n",
    "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
    "\n",
    "    def merge_from_dict(self, d):\n",
    "        self.__dict__.update(d)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    # a vanilla multi-head masked self-attention layer with a projection at the end (instead of using torch.nn.MultiheadAttention) the causal mask is applied to the attention scores to ensure that the model cannot \"cheat\" and look into the future when making predictions. the mask is applied to the attention scores before they are normalized with a softmax.  \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0 # make sure that the number of heads (6) evenly divides the embedding size (192)\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch. The self.c_attn is a nn.Linear module, where the input size is n_embd (the embedding size: 192) and the output size is 3 * n_embd. in the forward step, the output tensor will split into 3 tensors along the last dimension, each with shape (batch_size, seq_len, n_embd). These 3 tensors will be used as the query, key, and value vectors in the self-attention mechanism         \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # projection of the concatenated multi-head attention output back to embedding size. the output size of the self-attention layer is 3 * n_embd (3*192=576), but we want to project it back to n_embd (192)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # dropout regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        \n",
    "        # dropout regularization to the residual connection, which is the addition of the input to the output of the layer\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    # this is called from the forward method of the Block class. x is the input tensor of shape (batch_size, seq_len, 3 * n_embd) i.e. ([64, 128, 576])  \n",
    "    def forward(self, x): \n",
    "        B, T, C = x.size()  # B=Batch size (64), T=the sequence length (128), C=embedding size (192)\n",
    "        \n",
    "        # split the x input tensor along the last dimension into 3 tensors of size (batch_size, seq_len, n_embd)\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        # split the tensors along the second dimension into n_head tensors of size (batch_size, seq_len, n_embd // n_head) i.e. ([64, 128, 32]) \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) \n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) \n",
    "\n",
    "        # calculate attention scores (q @ k^T) and normalize them by dividing by the square root of the embedding size (192)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # apply the causal mask to the attention scores, before the softmax normalization\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "\n",
    "        # calculate attention weights by applying softmax normalization to the attention scores\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # apply dropout regularization to the attention weights\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # calculate the self-attention output as the weighted sum of the values (v), weighted by the attention weights (att)\n",
    "        y = att @ v # y has shape (batch_size, n_head, seq_len, n_embd // n_head) i.e. ([64, 6, 128, 32])\n",
    "\n",
    "        # re-assemble all head outputs side by side into a single tensor of shape (batch_size, seq_len, n_embd) i.e. ([64, 128, 192])\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  \n",
    "\n",
    "        # apply residual connection and projection (linear transoformation) to the input tensor y to finalize the self-attention layer\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # a single transformer block, which consists of a self-attention layer, a feed-forward network (MLP) and two Add & Norm layers\n",
    "    def __init__(self, config):\n",
    "        super().__init__()        \n",
    "        self.attn = CausalSelfAttention(config) # init the self-attention layer\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)        \n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            # c_fc is the first linear layer in the MLP, which expands the input size (n_embd = 192) 4 times, i.e. to a vector of size 4 * n_embd (768))\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            # c_proj is the second linear layer in the MLP, which maps the output of the first linear layer back to the original size of n_embd (192)\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            # act is the activation function used in the MLP. Here, act will apply the GELU activation function to the output of the FIRST linear layer of the MLP.  \n",
    "            act = NewGELU(),\n",
    "            # dropout layer, which in this case will apply to the output of the second linear layer (c_proj). \n",
    "            dropout = nn.Dropout(config.resid_pdrop)\n",
    "        ))\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        # assign self.mlp to 'm' so that we can use it in the forward function. \n",
    "        m = self.mlp\n",
    "        # mlpf - MLP forward - is the forward function of the MLP, which is a composition of the three layers in the MLP: c_fc, c_proj, and act. it is called in the forward function of the Block class. this lambda function is equivalent to the following function definition: \n",
    "        # def mlpf(x):\n",
    "        #     return m.dropout(m.c_proj(m.act(m.c_fc(x))))        \n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) \n",
    "\n",
    "    # this forward is called in the GPT class forward function\n",
    "    # it apply to the input x sequentially the following operations:  \n",
    "    # 1. normalization (pre-LN version - see above), \n",
    "    # 2. self-attention, \n",
    "    # 3. residual connection, \n",
    "    # 4. second normalization,\n",
    "    # 5. MLP, and    \n",
    "    # 6. second residual connection\n",
    "    # 7. return x as output of the block, which is the input to the next block\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) \n",
    "        x = x + self.mlpf(self.ln_2(x)) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):   \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = 6\n",
    "        C.n_head = 6\n",
    "        C.n_embd =  192\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        \n",
    "        # the transformer here is an nn.ModuleList container, which holds: h is a contrainer used to create a list of Block modules; wte is the token embedding layer; wpe is the position embedding layer; ln_f is the final layer normalization layer and drop is just a dropout layer\n",
    "        self.transformer = nn.ModuleDict(dict(            \n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),            \n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), \n",
    "            ln_f = nn.LayerNorm(config.n_embd),  \n",
    "        ))\n",
    "        #  the output of the transformer is the input to the linear layer lm_head, which outputs the logits vector with the probability of each word in the vocabulary to be the next word in the sequence. the size of the logits vector is the same as the size of the vocabulary \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper. torch.nn.apply() applies a given function to all the modules within a module container. The method takes two arguments: module - the module container on which the function will be applied; and func - the function to apply to each module. The function func is applied recursively to each submodule within the module container. The function should take a single module as input and should return the modified module. The output of torch.nn.apply() is the modified module container.         \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # next code is initializing the weight parameters for a specific layer in the model, specifically, it is initializing the weight matrix c_proj.weight. The named_parameters() method returns an iterator over the module's named parameters along with their corresponding values. The initialization is performed using the torch.nn.init.normal_() method, which sets the initial values of the tensor with random samples from a normal distribution. This scheme is known as the Xavier initialization, which aims to keep the scale of the gradients approximately the same throughout the training process.\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        #  This long function is unfortunately doing something very simple and is being very defensive: We are separating out all parameters of the model into two buckets: those that will experience weight decay for regularization and those that won't (biases, and layernorm/embedding weights). We are then returning the PyTorch optimizer object. separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name random note: because named_modules and named_parameters are recursive we will see the same tensors p many many times. but doing it this way allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        ''' note: we are using AdamW optimizer here, which is Adam with weight decay fix. The optimizer takes in a list of parameter groups optim_groups, which are the different subsets of model parameters that have different optimization settings such as learning rate. The optimizer will update the parameters in each group separately. '''\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    # this forward is called in the trainer: logits, self.loss = model(x, y). here idx is the input tensor, which holds indices of the input tokens, and targets is the output tensor (also indices of tokens). both are of shape (b=64, t=128) where b is the batch size and t is the sequence length (num of tokens). The targets are provided only in training mode, but not during inference. the forward method returns the logits and the loss. the logits output is a tensor of shape (b, t, vocab_size), and the loss is a scalar \n",
    "    def forward(self, idx, targets=None): \n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "\n",
    "        # This creates a tensor containing a sequence of integers starting from 0 to t with a step size of 1. The dtype argument specifies the data type of the tensor to be long, which means the values will be 64-bit integers. The unsqueeze(0) method call adds a new dimension of size 1 at the 0-th dimension of the tensor, to make the tensor compatible with other tensors in the model that have an additional batch dimension.        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)        \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)        \n",
    "        \n",
    "        # the token embeddings and positional embeddings are summed together by an element-wise addition to produce the input embeddings to the transformer blocks\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)        \n",
    "        \n",
    "        # feed the input embeddings into the first transformer block and use its output as input to the next block in a loop. block(x) is a call to the forward method of the Block class \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # apply the final layer norm    \n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # The output of the transformer blocks, x, has the shape (batch_size, sequence_length, n_embd) or (64, 128, 192). the final linear layer lm_head is applied to get the logits output tensor, which has the shape (batch_size, sequence_length, vocab_size) or (64, 128, 65). For each token in the input sequence, the model generates a vocabulary-sized vector (65) representing the predicted scores with respect to each token in the vocabulary. The input size of the nn.Linear layer lm_head is n_embd (192) and the output size is vocab_size (65)\n",
    "        logits = self.lm_head(x) #logits shape is (b, t, vocab_size) or (64, 128, 65)        \n",
    "\n",
    "        loss = None               \n",
    "        # in training mode, we have the targets and we can compute the loss. the loss is calculated on normalized logits after applying softmax operation, but if we use the PyTorch cross entropy loss function (F.cross_entropy), the Softmax operation is implemented internally and there is no need to explicitly apply softmax normalization on the logits. the output loss is a scalar\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # don't track gradients for this method\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        # take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete the sequence max_new_tokens times, feeding the predictions back into the model each time. \n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        CN = CfgNode\n",
    "        C = CN()\n",
    "        # device to train on\n",
    "        C.device = 'auto'\n",
    "        # dataloder parameters\n",
    "        C.num_workers = 4\n",
    "        # optimizer parameters\n",
    "        C.max_iters = 5000\n",
    "        C.batch_size = 64\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1 # only applied on matmul weights\n",
    "        C.grad_norm_clip = 1.0\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        print(\">>> init trainer\")\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "\n",
    "        # determine the device we'll train on\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback): # onevent is a string that specifies when the callback should be called. callback is a function that takes the trainer object as an argument. the callback function is added to the list of callbacks for the specified event. the callback is defined \n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "    \n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader, using the train_dataset that was passed in to the Trainer class. the train_dataset is a CharDataset object that is created in chargpt.py, in which the input tokens (x) and labels (y) are stored.\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)), # When replacement is set to True, the sampler can sample the same sample multiple times \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=config.batch_size, # number of sequences in each batch is 64\n",
    "            num_workers= 0 if platform.system() == 'Windows' else config.num_workers\n",
    "        )\n",
    "        # module.train() is a method in PyTorch's nn.Module class that sets the module to training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation modes, if they are affected, e.g. Dropout, BatchNorm, etc. \n",
    "        model.train()\n",
    "        # iteration counter and timer. each iteration is one batch of data\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        \n",
    "        # The iter() function is used to create an iterator object from a DataLoader object that can be used to get the next batch of data in the dataset. The next() function is then used on this iterator object to retrieve the next batch of data.\n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        # start the training loop\n",
    "        while True:\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed. The except StopIteration is used to catch the StopIteration exception that is raised by the next function when there is no more data to be returned from data_iter. If there is no more data to be returned, data_iter is reset to the beginning of the data by creating a new iterator from train_loader and the next batch of data is retrieved using next(data_iter). Finally, batch is created as a list of tensors that are transferred to the device (specified by self.device) using the to method\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter) # retrieve a new batch of sequences using data_iter, the iterator for the DataLoader object train_loader\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "\n",
    "            # each batch contains two tensors, x and y. x is the input token and y is the label (which is the next token in the input )sequence. x and y are of shape (batch_size, seq_len). \n",
    "            x, y = batch \n",
    "\n",
    "            # forward the model - this will call the GPT class (model) forward() method. \n",
    "            logits, self.loss = model(x, y)            \n",
    "\n",
    "            # backprop and update the parameters. model.zero_grad(set_to_none=True) is used to set the gradients of all the parameters of the model to zero before computing the gradients for the current batch.      \n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            # In PyTorch, calling backward() on a tensor computes the gradients of that tensor with respect to a scalar value\n",
    "            self.loss.backward()\n",
    "            \n",
    "            # clip the gradients and update the parameters to avoid the issue of exploding gradients during training\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "            \n",
    "            # the step() method updates the model parameters using the gradients that were computed during the backward pass, scaled by the learning rate and any other optimizer-specific scaling factors\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
    "                self.trigger_callbacks('on_finished_training')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Dataset class is responsible for loading the data and returning batches\n",
    "class CharDataset(Dataset):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        CN = CfgNode\n",
    "        C = CN()\n",
    "        C.block_size = 128\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        # set() is a python built-in that returns a unique list of characters\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        # create dictionaries to convert between characters and integers (stoi = string to integer) and between integers and characters (itos = integer to string)\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        # return as tensors\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    \n",
    "    CN = CfgNode\n",
    "    C = CN()\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 66    \n",
    "\n",
    "    # data\n",
    "    C.data = CharDataset.get_default_config()\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-mini'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file = 'input.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "with open(input_file, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")    \n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('train.bin')\n",
    "val_ids.tofile('val.bin')\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open('meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # get default config and overrides from the command line, if any\n",
    "    config = get_config()\n",
    "    set_seed(config.system.seed)\n",
    "\n",
    "    # construct the training dataset\n",
    "    text = open(input_file, 'r').read() \n",
    "    train_dataset = CharDataset(config.data, text)\n",
    "\n",
    "    # construct the model\n",
    "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "    config.model.block_size = train_dataset.get_block_size()\n",
    "    model = GPT(config.model)\n",
    "\n",
    "    # construct the trainer object\n",
    "    trainer = Trainer(config.trainer, model, train_dataset)\n",
    "\n",
    "    # iteration callback\n",
    "    def batch_end_callback(trainer):\n",
    "\n",
    "        if trainer.iter_num % 200 == 0:\n",
    "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "        if trainer.iter_num % 1000 == 0:\n",
    "            # evaluate both the train and test score\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # sample from the model...\n",
    "                context = \"In void of faith, \"\n",
    "                x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "                y = model.generate(x, 150, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "                completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "                print('\\n===>', completion, '<===\\n ')            \n",
    "            # revert model to training mode\n",
    "            model.train()        \n",
    "\n",
    "    # finished_training_callback\n",
    "    def finished_training_callback(trainer):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # sample from the model...\n",
    "            context = \"In void of faith, \"\n",
    "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "            clear_output(wait=True)  # Clear the output before printing the new value    \n",
    "            print('\\n', completion, '\\n ')                \n",
    "            time.sleep(1)  # Wait for 1 second  \n",
    "\n",
    "    # register the callbacks\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.set_callback('on_finished_training', finished_training_callback)\n",
    "\n",
    "    # run the optimization\n",
    "    trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
