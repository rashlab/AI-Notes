{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>**The Transformer**</mark>\n",
    "this notebook is based on the tiny shakespeare char-level GPT example in the [minGPT project](https://github.com/karpathy/minGPT) of Andrej Karpathy. \n",
    "\n",
    "[<i>Attention Is All You Need</i>](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017) introduced the Transformer, as -\n",
    "> _a model architecture eschewing recurrence and instead **relying entirely on an attention mechanism** to draw global dependencies between input and output. The Transformer allows for significantly more parallelization … the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution_\n",
    "The Transformer model was designed to address the limitations of recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in sequence-to-sequence tasks. The Transformer architecture eliminates the need for recurrent layers, enabling better parallelization, which significantly speeds up the training process.\n",
    "\n",
    "The Transformer was originally designed as a sequence-to-sequence translation model, where the **encoder** processes the input sequence in one language, and the **decoder** generates the output sequence in other language: \n",
    "\n",
    "<img src='https://github.com/rashlab/AI-Notes/raw/main/filez/encdec.png' width=350px>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <mark>**GPT**</mark>\n",
    "In 2018, OpenAI introduced GPT (Generative Pre-Trained Transformer) in [<i>Improving Language Understanding by Generative Pre-Training</i>](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford et al., 2018). \n",
    "The GPT model is based on the Transformer architecture. However, it modifies the original design by discarding the encoder and solely utilizing the decoder part. This adaptation is designed to **make GPT \"Autoregressive\" - in the generative phase, GPT predicts the next token in a sequence, conditioning on its own previous predictions**\n",
    "\n",
    "##### <font color='red'>It's important to note the differences between GPT's training mode and generative mode, as they serve distinct purposes and operate in unique ways</font>\n",
    "> __in training mode__, the objective of the model is to learn the language: the model is fed large amounts of text data and trained to predict the next token in the sequence, given the context of the previous tokens. in this mode, the <u>model weights are updated</u> through backpropagation, which minimizes the loss between the output vectors, called **logits**, and the actual tokens.\n",
    "\n",
    "> __in generative mode__, the model predicts the subsequent tokens of an initial sequence (user prompt) and then continues to generate additional tokens based on its previous predictions in an **autoregressive** process. The prediction is made by generating a probability distribution from the logits using softmax operation, and then selecting the token based on probability scores. in this mode, the <u>model weights are frowzen and not updated</u>\n",
    "\n",
    "__this is how GPT works in training mode:__\n",
    "\n",
    "* **Tokenization**: first, the input text data is tokenized into a sequence of tokens. In the original Transformer paper, the authors used a byte-pair encoding (BPE) tokenizer, which is similar to the WordPiece tokenizer used in BERT. In the GPT-3 paper, the authors used a byte-level BPE tokenizer, which is similar to the Byte-Pair Encoding (BPE) tokenizer used in GPT-2. In both cases, the tokenizer splits the input text into a sequence of tokens, where each token is a subword unit (e.g., a word, a character, or a subword). The tokenizer also adds special tokens to the beginning and end of the sequence, such as the [CLS] token for classification tasks and the [SEP] token for sentence pair classification tasks. In the original Transformer paper, the authors used the [PAD] token to pad the input sequences to a fixed length, while in the GPT-3 paper, the authors used the [EOS] token to mark the end of the sequence.\n",
    "\n",
    "* **Embeddings**: after tokenization, each token is converted into by an **embedding vector** that captures and represents the semantic information of the token. In the original GPT paper, the size of the embedding vector is 768; in GPT-3, the size is 1,248; the larger the embedding vector, the more information the model can capture about the language. \n",
    "\n",
    "* **Positional Encoding**: the embedding vectors are then passed through a positional encoding layer, which adds positional information to the embedding vectors. This positional information is important for the model to understand the order of the tokens in the input sequence. In the original Transformer paper, the authors used a sinusoidal function to compute the positional encoding. In the GPT-3 paper, the authors used a learned embedding matrix to compute the positional encoding. The resulting matrix, containing both the embedding vectors and positional encodings, is fed as input **(X)** into the Transformer's first self-attention block.\n",
    "\n",
    "* **Transformer Blocks** . GPT applies multiple **Transformer Blocks** over the embeddings of input sequences. Each block contains a **masked multi-headed self-attention layer** and a **pointwise feed-forward layer** (a feed-forward network - FFN), which consists of two linear (dense) layers with a non-linear activation function, such as GeLU (Gaussian Error Linear Units) in between. The purpose of this FFN is to introduce non-linearity into the model and combine features learned by the self-attention mechanism within the Transformer. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data.\n",
    "\n",
    "* Self-Attention is the fundamental operation of the Transformer. In the training phase, it is designed to weigh and relate the tokens of the input sequence to better capture the relationships and dependencies between them, by computing a score (or weight) for each pair of elements in the input sequence. These scores determine how much \"attention\" each element should pay to other elements in the sequence. Higher scores indicate stronger relationships between elements, while lower scores suggest weaker relationships. \n",
    "\n",
    "* The Self-Attention Mechanism transforms each token, represented by an embedding vector, into three vectors: a query vector Q, a key vector K, and a value vector V. This is done by applying linear transformations, specifically by multiplying the input sequence X with the corresponding weight matrices (W_Q, W_K, and W_V). The Q, K, and V vectors are then used to compute attention scores, which in turn are used to calculate the weighted sum of the values (V) to produce the output. [re Linear Transformation see [here](https://github.com/rashlab/AI-Notes/blob/main/nn.Linear/nn.Linear.ipynb)]\n",
    "\n",
    "```\n",
    "Q = X @ W_Q \n",
    "K = X @ W_K \n",
    "V = X @ W_V`\n",
    "```\n",
    "> Linear Transformation of the input sequence X to Q, K, V vectors\n",
    "\n",
    "<img src='../filez/qkv_dot.png' width=500/>\n",
    "\n",
    "\n",
    "\n",
    ">> note: the authors of <i>Attention Is All You Need</i> introduced the concept of Query/Key/Value, drawing an analogy to a retrieval system which, IMHO, is a bit confusing \n",
    "\n",
    "The first Self-Attention block transforms the input embedding sequence into a new embedding sequence, which is then passed through a series of additional self-attention blocks which refine the embeddings. This architecture designed to capture the relationships between each token and all the other tokens in the sequence, and eventrually \"*understand*\" the language. \n",
    "\n",
    "There is a difference between the working of the GPT model in training stage and in inference stage. \n",
    "\n",
    "At the final layer, the model computes logits for each token in the vocabulary, that represent the model's best understanding of how likely each token is to appear next in the given context, based on the patterns and relationships it has learned from the training data. After computing the logits, the model applies the softmax function to convert the logits into a probability distribution. This distribution can then be used to sample the next token or to select the most likely token, depending on the desired output.\n",
    "\n",
    "#### <mark>**Self-Attention**</mark>\n",
    "Self-attention is the fundamental operation of the Transformer. in the learning phase, it is designed to weigh and relate the tokens of the input sequence to better capture the relationships and dependencies between them, by computing a score (or weight) for each pair of elements in the input sequence. These scores determine how much attention each element should pay to other elements in the sequence. Higher scores indicate stronger relationships between elements, while lower scores suggest weaker relationships. By using these attention scores, the model can selectively focus on different parts of the input sequence, depending on their relevance to the task at \n",
    "\n",
    "The authors of Attention Is All You Need introduced the concept of Query/Key/Value vectors, or Q/K/V, as an analogy to a retrieval system (IMHO bad and confusing idea).\n",
    "the Q/K/V vectors are used to compute the attention scores, which are then used to compute the weighted sum of the values (V) to produce the output.\n",
    "##### __this is how it all works__\n",
    "* **Tokenization**: First, the input text is tokenized into a sequence of tokens. In the original Transformer paper, the authors used a byte-pair encoding (BPE) tokenizer, which is similar to the WordPiece tokenizer used in BERT. In the GPT-3 paper, the authors used a byte-level BPE tokenizer, which is similar to the Byte-Pair Encoding (BPE) tokenizer used in GPT-2. In both cases, the tokenizer splits the input text into a sequence of tokens, where each token is a subword unit (e.g., a word, a character, or a subword). The tokenizer also adds special tokens to the beginning and end of the sequence, such as the [CLS] token for classification tasks and the [SEP] token for sentence pair classification tasks. In the original Transformer paper, the authors used the [PAD] token to pad the input sequences to a fixed length, while in the GPT-3 paper, the authors used the [EOS] token to mark the end of the sequence.\n",
    "\n",
    "* **Embeddings**: Each token is then converted into an embedding vector, which is a continuous representation that captures the semantic information of the token. In the original GPT paper, the size of the embedding vector is 768; in GPT-3, the size is 1,248; the larger the embedding vector, the more information the model can capture about the language. \n",
    "\n",
    "* **Positional Encoding**: The embedding vectors are then passed through a positional encoding layer, which adds positional information to the embedding vectors. This positional information is important for the model to understand the order of the tokens in the input sequence. In the original Transformer paper, the authors used a sinusoidal function to compute the positional encoding. In the GPT-3 paper, the authors used a learned embedding matrix to compute the positional encoding.\n",
    "\n",
    "* **Self-Attention** The resulting matrix, containing both the embedding vectors and positional encodings, is fed as input **X** into the Transformer's first self-attention block. Here, each token in the input sequence is transformed into three vectors: a query vector Q, a key vector K, and a value vector V. These transformations are achieved by applying **linear transformations** to the input sequence X using learned weight matrices (W_Q, W_K, and W_V). The Q, K, and V vectors are then used to compute attention scores, which in turn are used to calculate the weighted sum of the values (V) to produce the output. [re Linear Transformation see [here](https://github.com/rashlab/AI_School/blob/main/nn.Linear/about_nn.Linear.ipynb)]\n",
    "\n",
    ", which are then used to compute the attention weights. The `Q`, `K`, and `V` tensors are calculated by multiplying the input sequence `x` by learned weight matrices `W_Q`, `W_K`, and `W_V`, respectively. \n",
    "```\n",
    "# transforming the input sequence x to Q, K, V\n",
    "Q = X @ W_Q \n",
    "K = X @ W_K \n",
    "V = X @ W_V`\n",
    "```\n",
    "Here, `@` represents the matrix multiplication operator. The `Q`, `K`, and `V` tensors have the same shape, which is `(batch_size, sequence_length, hidden_size)`, where `batch_size` is the number of input sequences in the batch, `sequence_length` is the length of each input sequence, and `hidden_size` is the size of the hidden representation (embeddings).\n",
    "\n",
    "The attention scores are calculated by using dot product between each token and each of the other tokens in the sequence, for measuring the strength of their relationship. \n",
    "\n",
    "\n",
    ". s vectors (weights) `A`  are computed by taking the dot product of the query tensor `Q` with the transpose of the key tensor `K`, and then applying a softmax activation function to the result:\n",
    "\n",
    "```\n",
    "A (Attention) = softmax(Q @ K.transpose(-1, -2) / sqrt(d_k))\n",
    "```\n",
    "\n",
    "Here, `softmax` is the softmax activation function, `transpose` transposes the tensor along the last two dimensions (so that the dot product is calculated across the `hidden_size` (embedding) dimension), and `sqrt(d_k)` is a scaling factor that helps to prevent the dot product from becoming too large. The resulting attention weights `A` have the same shape as `Q` and `K`, which is `(batch_size, sequence_length, sequence_length)`.\n",
    "\n",
    "The final output of the attention mechanism is a weighted sum of the value tensor `V`, where the weights are given by the attention weights `A`:\n",
    "```\n",
    "O (Output) = A @ V\n",
    "```\n",
    "\n",
    "Here, `O` is the output of the attention mechanism, and has the same shape as `Q`, `K`, and `V`, which is `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "In summary, the attention mechanism in GPT uses learned weight matrices to transform the input sequence into query, key, and value tensors, and then uses the dot product of the query and key tensors to compute attention weights, which are used to compute a weighted sum of the value tensor to produce the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the next step is to pass the embedding vectors through a series of self-attention blocks, which are the fundamental building blocks of the Transformer. Each self-attention block consists of a masked multi-headed self-attention layer and a pointwise feed-forward layer (a feed-forward network - FFN), which consists of two linear (dense) layers with a non-linear activation function, such as GeLU (Gaussian Error Linear Units) in between. The purpose of this FFN is to introduce non-linearity into the model and combine features learned by the self-attention mechanism within the Transformer. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear activation function helps the model capture complex relationships in the data. The first linear layer of the FFN increases the dimensionality of the input, while the second linear layer reduces it back to the original dimension. The non-linear\n",
    "\n",
    "\n",
    "\n",
    "> * the embedding vector is a vector of numbers that represents and captures the semantics of each token.\n",
    "The input tokens are r\n",
    "is first passed through a linear layer to produce Q, K, and V vectors.\n",
    "\n",
    "\n",
    "Self-attention has several advantages in NLP tasks, such as:\n",
    "* Capturing long-range dependencies: Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), self-attention can directly capture relationships between elements that are far apart in the input sequence, which is particularly useful for understanding complex sentences.\n",
    "* Parallelization: The self-attention mechanism can process all elements of the input sequence simultaneously, which allows for more efficient computation compared to RNNs, where elements must be processed sequentially.\n",
    "* Interpretability: The attention scores provide a degree of interpretability, as they can show which parts of the input sequence are most relevant to the model's predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports..\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import platform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils..\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"  Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415. GELU is a smooth approximation of the ReLU function \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class CfgNode:\n",
    "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._str_helper(0)\n",
    "\n",
    "    def _str_helper(self, indent):\n",
    "        parts = []\n",
    "        for k, v in self.__dict__.items():\n",
    "            if isinstance(v, CfgNode):\n",
    "                parts.append(\"%s:\\n\" % k)\n",
    "                parts.append(v._str_helper(indent + 1))\n",
    "            else:\n",
    "                parts.append(\"%s: %s\\n\" % (k, v))\n",
    "        parts = [' ' * (indent * 4) + p for p in parts]\n",
    "        return \"\".join(parts)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\" return a dict representation of the config \"\"\"\n",
    "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
    "\n",
    "    def merge_from_dict(self, d):\n",
    "        self.__dict__.update(d)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    ''' a vanilla multi-head masked self-attention layer with a projection at the end (instead of using torch.nn.MultiheadAttention) the causal mask is applied to the attention weights to ensure that the model cannot \"cheat\" and look into the future when making predictions. the mask is applied to the attention weights before they are normalized with a softmax.  '''\n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0 # make sure that the number of heads evenly divides the embedding size\n",
    "        ''' key, query, value projections for all heads, but in a batch. The self.c_attn is a nn.Linear module, where the input size is config.n_embd (the token embedding size) and the output size is 3 * config.n_embd. In the forward step, the output tensor will split into 3 tensors along the last dimension, each with shape (batch_size, seq_len, config.n_embd). These 3 tensors will be used as the query, key, and value vectors in the self-attention mechanism '''\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # projection of the concatenated multi-head attention output back to embedding size (i.e. the output size of the self-attention layer is 3 * config.n_embd, but we want to project it back to config.n_embd, which is the size of the token embeddings)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # dropout regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        # next is a dropout regularization to the residual connection, which is a bypass that allows the gradient to flow more easily through the network\n",
    "        # the residual connection is the addition of the input to the output of the layer\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    ''' this is called in the forward method of the Block class. x is the input tensor of shape (batch_size, seq_len, 3 * config.n_embd) i.e. ([64, 128, 576g]) in this tiny shakespear example '''\n",
    "    def forward(self, x): \n",
    "        B, T, C = x.size()  # B=Batch size, T=block size or sequence length, i.e. the number of Tokens, C=num of Channels or embedding size\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # we split the x input tensor along the last dimension into 3 tensors of size (batch_size, seq_len, config.n_embd)\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        # now we split the tensors along the second dimension into n_head tensors of size (batch_size, seq_len, config.n_embd // n_head)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # here we apply the causal mask to the attention weights, before the softmax normalization\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # apply residual connection and projection (linear transoformation) to the input tensor y to finalize the self-attention layer\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    ''' a single block of the transformer model '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config) # init the self-attention layer\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            # c_fc is the first linear layer in the MLP, which expands the input to a larger vector size\n",
    "            # Specifically, it maps from a vector of size `config.n_embd` to a larger vector of size 4 * `config.n_embd`\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            # c_proj is the second linear layer in the MLP, which maps the output of the first linear layer back to the original size of `config.n_embd`\n",
    "            # Specifically, it maps from a larger vector of size 4 * `config.n_embd` to a vector of size `config.n_embd`\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            # act is the activation function used in the MLP. Here, act will apply the GELU activation function to the output of the FIRST linear layer in this MLP. The purpose of this is to introduce non-linearity to the self-attention computation. The self-attention mechanism is essentially a weighted sum of the input tokens, and without non-linearity, it may not be able to capture complex relationships between the tokens. \n",
    "            act = NewGELU(),\n",
    "            # dropout layer, which in this case will apply to the output of the second linear layer (c_proj). \n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        # assign self.mlp to 'm' so that we can use it in the forward function. \n",
    "        m = self.mlp\n",
    "        # mlpf is the forward function of the MLP, which is a composition of the three layers in the MLP: c_fc, c_proj, and act\n",
    "        # the lambda function is a shorthand for defining a function in Python. in this case, it is equivalent to the following: \n",
    "        # def mlpf(x):\n",
    "        #     return m.dropout(m.c_proj(m.act(m.c_fc(x))))        \n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward - it is called in the forward function of the Block class\n",
    "\n",
    "    # this is called in the GPT class forward function\n",
    "    # it first applies the self-attention mechanism to the input, \n",
    "    # and then applies the MLP to the output of the self-attention mechanism\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) # this is calling the forward method of the CausalSelfAttention class. the addition of x is the residual connection\n",
    "        x = x + self.mlpf(self.ln_2(x)) # this is calling the mlpf function defined above. the addition of x is the residual connection\n",
    "        return x # the output of the block is the input to the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):   \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = 6\n",
    "        C.n_head = 6\n",
    "        C.n_embd =  192\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        \n",
    "        # transformer is an nn.ModuleList container, and h is a contrainer used to create a list of Block modules. wte is the token embedding layer, wpe is the position embedding layer the position embedding layer is used to encode the position of a token in the sequence. \n",
    "        self.transformer = nn.ModuleDict(dict(            \n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),            \n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), \n",
    "            ln_f = nn.LayerNorm(config.n_embd),  \n",
    "        ))\n",
    "        #  the output of the transformer is the input to the linear layer lm_head. \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        ''' init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper. torch.nn.apply() applies a given function to all the modules within a module container. The method takes two arguments: module - the module container on which the function will be applied; and func - the function to apply to each module. The function func is applied recursively to each submodule within the module container. The function should take a single module as input and should return the modified module. The output of torch.nn.apply() is the modified module container.         '''\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        '''next code is initializing the weight parameters for a specific layer in the model, specifically, it is initializing the weight matrix c_proj.weight. The named_parameters() method returns an iterator over the module's named parameters along with their corresponding values. The initialization is performed using the torch.nn.init.normal_() method, which sets the initial values of the tensor with random samples from a normal distribution. This scheme is known as the Xavier initialization, which aims to keep the scale of the gradients approximately the same throughout the training process.'''\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"  This long function is unfortunately doing something very simple and is being very defensive: We are separating out all parameters of the model into two buckets: those that will experience weight decay for regularization and those that won't (biases, and layernorm/embedding weights). We are then returning the PyTorch optimizer object. \"\"\"\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name random note: because named_modules and named_parameters are recursive we will see the same tensors p many many times. but doing it this way allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        ''' note: we are using AdamW optimizer here, which is Adam with weight decay fix. The optimizer takes in a list of parameter groups optim_groups, which are the different subsets of model parameters that have different optimization settings such as learning rate. The optimizer will update the parameters in each group separately. '''\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    # in this forward method of the GPT module class, idx is the input tensor (indices of tokens), and targets is the output tensor (also indices of tokens). For our tiny shakespeares, both are of shape (b=64, t=128) where b is the batch size and t is the sequence length (num of chars). \n",
    "    # this forward is called by the trainer in logits, self.loss = model(x, y)\n",
    "    def forward(self, idx, targets=None): \n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        # This creates a tensor containing a sequence of integers starting from 0 to t with a step size of 1. The dtype argument specifies the data type of the tensor to be long, which means the values will be 64-bit integers. The unsqueeze(0) method call adds a new dimension of size 1 at the 0-th dimension of the tensor, to make the tensor compatible with other tensors in the model that have an additional batch dimension.\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)        \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)        \n",
    "        # tok_emb and pos_emb are the token and positional embeddings, respectively, that are summed together to produce the final input embeddings to the transformer blocks. The drop method applies a dropout regularization - it works by randomly setting a fraction of the input tensor's values to zero during each training iteration. The drop method takes a single argument x and returns a tensor with the same shape as x, but with some of its elements randomly set to zero. tok_emb and pos_emb have the same shape, and when you add them together, you are performing an element-wise addition between the corresponding elements of the two tensors\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)        \n",
    "        # now we feed the embeddings into the transformer blocks contained in self.transformer.h. block(x) is a call to the forward method of the Block class \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # apply the final layer norm and the linear layer to get the logits. the logits are the output of the linear layer of shape (batch_size, n_embed, vocab_size), where each element of the tensor represents the predicted scores for each word in the vocabulary at each position in the input sequence.       \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss. \n",
    "        # The cross_entropy function takes two arguments: the logits and the targets. The logits are the output of the linear layer, and the targets are the desired output values. In this case, logits is a tensor of shape (batch_size, seq_length, vocab_size) containing the predicted logits for each possible output token at each position in the sequence, and targets is a tensor of shape (batch_size, seq_length) containing the true (index of) target tokens for each position in the sequence. The method view(-1, logits.size(-1)) is used to reshape the logits tensor to have shape (batch_size * seq_length, vocab_size). This is necessary because the F. cross_entropy() function expects the logits to have shape (N, C) where N is the total number of predictions (i.e., batch_size * seq_length) and C is the number of classes (i.e., vocab_size). The ignore_index parameter is used to specify a token index that should be ignored in the loss calculation. In this case, -1 is specified to ignore any padding tokens in the targets tensor. The resulting loss is a scalar tensor that represents the average cross-entropy loss over all tokens in the batch.\n",
    "        loss = None               \n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # don't track gradients for this method\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\" take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete the sequence max_new_tokens times, feeding the predictions back into the model each time. \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        CN = CfgNode\n",
    "        C = CN()\n",
    "        # device to train on\n",
    "        C.device = 'auto'\n",
    "        # dataloder parameters\n",
    "        C.num_workers = 4\n",
    "        # optimizer parameters\n",
    "        C.max_iters = 5000\n",
    "        C.batch_size = 64\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1 # only applied on matmul weights\n",
    "        C.grad_norm_clip = 1.0\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        print(\">>> init trainer\")\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "\n",
    "        # determine the device we'll train on\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback): # onevent is a string that specifies when the callback should be called. callback is a function that takes the trainer object as an argument. the callback function is added to the list of callbacks for the specified event. the callback is defined \n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "    \n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader, using the train_dataset that was passed in to the Trainer class. the train_dataset is a CharDataset object that is created in chargpt.py, in which the input tokens (x) and labels (y) are stored.\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)), # When replacement is set to True, the sampler can sample the same sample multiple times \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers= 0 if platform.system() == 'Windows' else config.num_workers\n",
    "        )\n",
    "        ''' module.train() is a method in PyTorch's nn.Module class that sets the module to training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation modes, if they are affected, e.g. Dropout, BatchNorm, etc. '''\n",
    "        model.train()\n",
    "        # iteration counter and timer. each iteration is one batch of data\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        ''' The iter() function is used to create an iterator object from a DataLoader object that can be used to get the next batch of data in the dataset. The next() function is then used on this iterator object to retrieve the next batch of data.'''\n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        while True:\n",
    "            ''' fetch the next batch (x, y) and re-init iterator if needed. The except StopIteration is used to catch the StopIteration exception that is raised by the next function when there is no more data to be returned from data_iter. If there is no more data to be returned, data_iter is reset to the beginning of the data by creating a new iterator from train_loader and the next batch of data is retrieved using next(data_iter). Finally, batch is created as a list of tensors that are transferred to the device (specified by self.device) using the to method. '''\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "\n",
    "            ''' each batch contains two tensors, x and y. x is the input token and y is the label. x and y are of shape (batch_size, seq_len). '''\n",
    "            x, y = batch \n",
    "            ''' forward the model - this will call the GPT class (model) forward() method. the loss returned is a scalar tensor that represents the average cross-entropy loss over all tokens in the batch. logits is a tensor of shape (batch_size, n_embed, vocab_size) that contains the predicted scores for each class in the output vocabulary. '''\n",
    "            logits, self.loss = model(x, y)\n",
    "\n",
    "            '''backprop and update the parameters. model.zero_grad(set_to_none=True) is used to set the gradients of all the parameters of the model to zero before computing the gradients for the current batch.      '''\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            '''In PyTorch, calling backward() on a tensor computes the gradients of that tensor with respect to a scalar value.       '''\n",
    "            self.loss.backward()\n",
    "            \n",
    "            # clip the gradients and update the parameters to avoid the issue of exploding gradients during training. \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "            \n",
    "            ''' the step() method updates the model parameters using the gradients that were computed during the backward pass, scaled by the learning rate and any other optimizer-specific scaling factors.'''\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
    "                self.trigger_callbacks('on_finished_training')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    \n",
    "    CN = CfgNode\n",
    "    C = CN()\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 3407    \n",
    "\n",
    "    # data\n",
    "    C.data = CharDataset.get_default_config()\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-mini'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "\n",
    "    return C\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# the Dataset class is responsible for loading the data and returning batches\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        CN = CfgNode\n",
    "        C = CN()\n",
    "        C.block_size = 128\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "        # set() is a python built-in that returns a unique list of characters\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        # create dictionaries to convert between characters and integers (stoi = string to integer) and between integers and characters (itos = integer to string)\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        # return as tensors\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file = 'input.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "with open(input_file, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('train.bin')\n",
    "val_ids.tofile('val.bin')\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open('meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # get default config and overrides from the command line, if any\n",
    "    config = get_config()\n",
    "    set_seed(config.system.seed)\n",
    "\n",
    "    # construct the training dataset\n",
    "    text = open(input_file, 'r').read() \n",
    "    train_dataset = CharDataset(config.data, text)\n",
    "\n",
    "    # construct the model\n",
    "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "    config.model.block_size = train_dataset.get_block_size()\n",
    "    model = GPT(config.model)\n",
    "\n",
    "    # construct the trainer object\n",
    "    trainer = Trainer(config.trainer, model, train_dataset)\n",
    "\n",
    "    # iteration callback\n",
    "    def batch_end_callback(trainer):\n",
    "\n",
    "        if trainer.iter_num % 200 == 0:\n",
    "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "        if trainer.iter_num % 1000 == 0:\n",
    "            # evaluate both the train and test score\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # sample from the model...\n",
    "                context = \"O God, O God!\"\n",
    "                x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "                y = model.generate(x, 150, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "                completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "                print(completion)            \n",
    "            # revert model to training mode\n",
    "            model.train()        \n",
    "\n",
    "    # finished_training_callback\n",
    "    def finished_training_callback(trainer):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # sample from the model...\n",
    "            context = \"O God, O God!\"\n",
    "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "            clear_output(wait=True)  # Clear the output before printing the new value    \n",
    "            print(completion)            \n",
    "            time.sleep(1)  # Wait for 1 second  \n",
    "\n",
    "    # register the callbacks\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.set_callback('on_finished_training', finished_training_callback)\n",
    "\n",
    "    # run the optimization\n",
    "    trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
